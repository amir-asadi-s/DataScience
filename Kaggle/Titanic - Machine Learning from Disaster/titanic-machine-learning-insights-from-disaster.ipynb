{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic machine learning insights from disaster","metadata":{"_uuid":"59d1fed8-fed8-4a60-86e8-02bafddb13c4","_cell_guid":"0c8c4e8e-2cc3-4683-8753-a54648f5a19b","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"_uuid":"e98999b9-f073-4147-9a05-00e04b4f9970","_cell_guid":"5326de2b-1a7c-4dd9-ad49-18f50e01775b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-24T12:28:55.570909Z","iopub.execute_input":"2024-10-24T12:28:55.572171Z","iopub.status.idle":"2024-10-24T12:28:55.579525Z","shell.execute_reply.started":"2024-10-24T12:28:55.572122Z","shell.execute_reply":"2024-10-24T12:28:55.578457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 1: Load and Explore Data","metadata":{"_uuid":"f7ff1590-4120-4d36-aa2b-e67b2004e495","_cell_guid":"239d405c-9223-42df-8f1d-b16257e2f42b","trusted":true}},{"cell_type":"code","source":"def load_data(train_file, test_file):\n    \"\"\"\n    Load the Titanic dataset.\n    \n    Args:\n    train_file: path to the training dataset\n    test_file: path to the test dataset\n    \n    Returns:\n    df_train: Training DataFrame\n    df_test: Test DataFrame\n    \"\"\"\n    df_train = pd.read_csv(train_file)\n    df_test = pd.read_csv(test_file)\n    \n    print(f\"Training Data Shape: {df_train.shape}\")\n    print(f\"Test Data Shape: {df_test.shape}\")\n    \n    return df_train, df_test","metadata":{"_uuid":"fd2940ec-c47b-46e4-839d-f95645e655eb","_cell_guid":"dd3d82dd-83d8-4d63-bfb8-2c1544ce13a3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-24T12:28:55.581753Z","iopub.execute_input":"2024-10-24T12:28:55.582138Z","iopub.status.idle":"2024-10-24T12:28:55.601557Z","shell.execute_reply.started":"2024-10-24T12:28:55.582080Z","shell.execute_reply":"2024-10-24T12:28:55.600429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Data Preprocessing","metadata":{"_uuid":"1a2fa740-b02a-472b-9d20-7b1bf21b1383","_cell_guid":"0d2aa276-31a4-46f3-99ae-8d837252b328","trusted":true}},{"cell_type":"code","source":"def preprocess_data(df_train, df_test):\n    \"\"\"\n    Preprocess the Titanic dataset by handling missing values, feature engineering, and one-hot encoding.\n    \n    Args:\n    df_train: DataFrame containing the training data\n    df_test: DataFrame containing the test data\n    \n    Returns:\n    processed_train: Preprocessed training dataset\n    processed_test: Preprocessed test dataset with PassengerId preserved\n    \"\"\"\n    # Preserve PassengerId for the test dataset\n    passenger_ids = df_test['PassengerId']\n    \n    # Combine the datasets for uniform preprocessing\n    df_test['Survived'] = np.nan\n    df_combined = pd.concat([df_train, df_test], axis=0)\n    \n    # Drop unnecessary columns\n    df_combined = df_combined.drop(['PassengerId', 'Name', 'Ticket'], axis=1)\n    \n    # Handle missing values\n    df_combined['Age'] = df_combined['Age'].fillna(df_combined['Age'].mean())\n    df_combined['Fare'] = df_combined['Fare'].fillna(df_combined['Fare'].mean())\n    df_combined['Embarked'] = df_combined['Embarked'].fillna('S')\n    df_combined['Cabin'] = df_combined['Cabin'].fillna('X000')\n    \n    # Feature Engineering: Cabin letter and cabin number\n    df_combined['cabin_letter'] = df_combined['Cabin'].str.extract(r'([a-zA-Z]+)', expand=False)\n    df_combined['cabin_number'] = df_combined['Cabin'].str.extract(r'(\\d+)', expand=False).fillna(0).astype(int)\n    df_combined = df_combined.drop('Cabin', axis=1)\n\n    # One-hot encoding of categorical variables\n    df_combined = pd.get_dummies(df_combined, columns=['Sex', 'Embarked', 'cabin_letter'], drop_first=True)\n    \n    # Interaction features\n    df_combined['Pclass_bin_Fare'] = df_combined['Fare'] // df_combined['Pclass']\n    df_combined['Pclass_bin_sex'] = df_combined['Pclass'] - df_combined['Sex_male']\n\n    # Split the combined data back into train and test sets\n    processed_train = df_combined[df_combined['Survived'].notna()].copy()\n    processed_test = df_combined[df_combined['Survived'].isna()].copy()\n    \n    # Drop 'Survived' from the test set\n    processed_train['Survived'] = processed_train['Survived'].astype(int)\n    processed_test = processed_test.drop('Survived', axis=1)\n    \n    # Restore PassengerId for the test set\n    processed_test['PassengerId'] = passenger_ids\n    \n    return processed_train, processed_test","metadata":{"_uuid":"d9ab269c-828d-4e15-85c7-46cd71c6dbc7","_cell_guid":"534094da-0ed3-42c8-9b2c-0b6a54f33c40","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-24T12:28:55.603681Z","iopub.execute_input":"2024-10-24T12:28:55.604168Z","iopub.status.idle":"2024-10-24T12:28:55.619388Z","shell.execute_reply.started":"2024-10-24T12:28:55.604105Z","shell.execute_reply":"2024-10-24T12:28:55.618279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Exploratory Data Analysis (EDA) - Key Visualizations","metadata":{"_uuid":"a2934857-f478-45df-8a15-6cd104b8ceb7","_cell_guid":"696cc814-33f8-42f5-84a7-ac8ecad4b395","trusted":true}},{"cell_type":"code","source":"def create_key_visualizations(df_train):\n    \"\"\"\n    Generate key visualizations for the Titanic dataset.\n    \n    Args:\n    df_train: DataFrame containing the training data\n    \n    Returns:\n    None (displays plots)\n    \"\"\"\n    # Set plot style\n    sns.set(style=\"whitegrid\")\n\n    # 1. Survival Rate by Gender\n    plt.figure(figsize=(8, 6))\n    sns.barplot(x='Sex', y='Survived', data=df_train)\n    plt.title('Survival Rate by Gender')\n    plt.ylabel('Survival Rate')\n    plt.xlabel('Gender')\n    plt.show()\n\n    # 2. Survival Rate by Passenger Class\n    plt.figure(figsize=(8, 6))\n    sns.barplot(x='Pclass', y='Survived', data=df_train)\n    plt.title('Survival Rate by Passenger Class')\n    plt.ylabel('Survival Rate')\n    plt.xlabel('Passenger Class')\n    plt.show()\n\n    # 3. Age Distribution of Survivors and Non-Survivors\n    plt.figure(figsize=(10, 8))\n    sns.histplot(df_train[df_train['Survived'] == 1]['Age'], bins=20, label='Survived', kde=True, color='green')\n    sns.histplot(df_train[df_train['Survived'] == 0]['Age'], bins=20, label='Did not survive', kde=True, color='red')\n    plt.title('Age Distribution of Survivors and Non-Survivors')\n    plt.xlabel('Age')\n    plt.legend()\n    plt.show()\n\n    # 4. Survival Rate by Embarkation Point\n    plt.figure(figsize=(8, 6))\n    sns.barplot(x='Embarked', y='Survived', data=df_train)\n    plt.title('Survival Rate by Embarkation Point')\n    plt.ylabel('Survival Rate')\n    plt.xlabel('Embarkation Point')\n    plt.show()","metadata":{"_uuid":"123245bb-46c9-4536-a792-d9acb4ef1a83","_cell_guid":"2cdb4fbf-2f03-44f4-af2d-3a91f9fe5152","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-24T12:28:55.620978Z","iopub.execute_input":"2024-10-24T12:28:55.621932Z","iopub.status.idle":"2024-10-24T12:28:55.636565Z","shell.execute_reply.started":"2024-10-24T12:28:55.621875Z","shell.execute_reply":"2024-10-24T12:28:55.635535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Split the Data and Apply Feature Scaling","metadata":{"_uuid":"ef037f0b-fb22-4558-add1-7944228c3f1c","_cell_guid":"df66091b-2259-4675-b80e-39ebc7231547","trusted":true}},{"cell_type":"code","source":"def split_and_scale_data(processed_train):\n    \"\"\"\n    Split the training data into train and validation sets, and scale the features.\n    \n    Args:\n    processed_train: Preprocessed training dataset\n    \n    Returns:\n    X_train_scaled, X_val_scaled, y_train, y_val: Scaled training and validation features and labels\n    scaler: Fitted scaler object\n    \"\"\"\n    X = processed_train.drop(['Survived'], axis=1)\n    y = processed_train['Survived']\n    \n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Feature scaling\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    return X_train_scaled, X_val_scaled, y_train, y_val, scaler","metadata":{"_uuid":"0f338fed-68f5-40d2-87a6-64442bf35dcf","_cell_guid":"911b4f7b-1bca-4b55-ae0f-55780d283b7f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-24T12:28:55.639453Z","iopub.execute_input":"2024-10-24T12:28:55.640171Z","iopub.status.idle":"2024-10-24T12:28:55.652395Z","shell.execute_reply.started":"2024-10-24T12:28:55.640130Z","shell.execute_reply":"2024-10-24T12:28:55.651486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Model Training and Tuning","metadata":{"_uuid":"bb2e2fe2-2fac-42e0-bd9f-0a398a007b99","_cell_guid":"179cc567-77ca-49fc-ac85-79fd22365760","trusted":true}},{"cell_type":"code","source":"# Logistic Regression Model\ndef train_logistic_regression(X_train_scaled, y_train):\n    \"\"\"\n    Train a Logistic Regression model.\n    \n    Args:\n    X_train_scaled: Scaled training features\n    y_train: Training labels\n    \n    Returns:\n    log_model: Trained Logistic Regression model\n    \"\"\"\n    log_model = LogisticRegression(max_iter=1000, random_state=42)\n    log_model.fit(X_train_scaled, y_train)\n    return log_model\n\n# Random Forest Model with Hyperparameter Tuning\ndef tune_random_forest(X_train_scaled, y_train):\n    \"\"\"\n    Perform GridSearchCV to tune Random Forest hyperparameters.\n    \n    Args:\n    X_train_scaled: Scaled training features\n    y_train: Training labels\n    \n    Returns:\n    best_rf_model: Tuned Random Forest model\n    \"\"\"\n    param_grid = {\n        'n_estimators': [100, 200,300 ],\n        'max_depth': [20, 30, 40],\n        'min_samples_split': [2, 3, 14],\n        'min_samples_leaf': [2, 3, 4]\n    }\n    rf = RandomForestClassifier(random_state=42)\n    grid_search = GridSearchCV(rf, param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n    grid_search.fit(X_train_scaled, y_train)\n    print(f\"Best params for Random Forest: {grid_search.best_params_}\")\n    return grid_search.best_estimator_\n\n# XGBoost Model with Hyperparameter Tuning\ndef tune_xgboost(X_train_scaled, y_train):\n    \"\"\"\n    Perform GridSearchCV to tune XGBoost hyperparameters.\n    \n    Args:\n    X_train_scaled: Scaled training features\n    y_train: Training labels\n    \n    Returns:\n    best_xgb_model: Tuned XGBoost model\n    \"\"\"\n    param_grid = {\n        'n_estimators': [50, 100, 150],\n        'max_depth': [4, 5, 6],\n        'learning_rate': [0.01, 0.05, 0.1]\n    }\n    xgb = XGBClassifier(objective='binary:logistic', random_state=42)\n    grid_search = GridSearchCV(xgb, param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n    grid_search.fit(X_train_scaled, y_train)\n    print(f\"Best params for XGBoost: {grid_search.best_params_}\")\n    return grid_search.best_estimator_","metadata":{"_uuid":"2910008d-be0f-492d-89ed-2a06dcd9bf4f","_cell_guid":"77e9bc50-48f7-4b88-8bbc-bd98816d04d3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-24T12:28:55.702066Z","iopub.execute_input":"2024-10-24T12:28:55.702509Z","iopub.status.idle":"2024-10-24T12:28:55.714422Z","shell.execute_reply.started":"2024-10-24T12:28:55.702468Z","shell.execute_reply":"2024-10-24T12:28:55.713137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 6: Feature Importance (Random Forest)","metadata":{"_uuid":"db25b44d-fb7a-4c8b-952a-86ae3f886fea","_cell_guid":"78a0f3e6-ddc8-411b-aca0-ff9dca154a6a","trusted":true}},{"cell_type":"code","source":"def plot_feature_importance(rf_model, X_train):\n    \"\"\"\n    Plot feature importance from the trained Random Forest model.\n    \n    Args:\n    rf_model: Trained Random Forest model\n    X_train: Training dataset (features only, not including target)\n    \n    Returns:\n    None (displays the plot)\n    \"\"\"\n    # Get feature importance from the Random Forest model\n    feature_importance = rf_model.feature_importances_\n    \n    # Create a DataFrame for visualization\n    features = X_train.columns\n    importance_df = pd.DataFrame({\n        'Feature': features,\n        'Importance': feature_importance\n    }).sort_values(by='Importance', ascending=False)\n    \n    # Plot feature importance\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x='Importance', y='Feature', data=importance_df)\n    plt.title('Feature Importance from Random Forest')\n    plt.show()","metadata":{"_uuid":"b26524db-6df5-4047-8f45-30ed6fb596d4","_cell_guid":"abdc7124-5348-4d36-8720-0b4045c20261","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-24T12:28:55.717006Z","iopub.execute_input":"2024-10-24T12:28:55.717501Z","iopub.status.idle":"2024-10-24T12:28:55.730451Z","shell.execute_reply.started":"2024-10-24T12:28:55.717445Z","shell.execute_reply":"2024-10-24T12:28:55.729333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 7: Model Evaluation and Best Model Selection","metadata":{"_uuid":"10fba24d-7ecd-47a6-af02-977f85292e79","_cell_guid":"d295e26a-1e7c-42c0-9a95-43b5e25ef11c","trusted":true}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n\ndef evaluate_models(X_val_scaled, y_val, log_model, rf_model, xgb_model):\n    \"\"\"\n    Evaluate the performance of Logistic Regression, Random Forest, and XGBoost models on the validation set.\n    \n    Args:\n    X_val_scaled: Scaled validation features\n    y_val: Validation labels\n    log_model: Trained Logistic Regression model\n    rf_model: Trained Random Forest model\n    xgb_model: Trained XGBoost model\n    \n    Returns:\n    best_model: Model with the highest validation accuracy\n    \"\"\"\n    def print_metrics(y_true, y_pred, model_name):\n        \"\"\"\n        Print evaluation metrics for a model.\n        \"\"\"\n        accuracy = accuracy_score(y_true, y_pred)\n        precision = precision_score(y_true, y_pred)\n        recall = recall_score(y_true, y_pred)\n        f1 = f1_score(y_true, y_pred)\n        roc_auc = roc_auc_score(y_true, y_pred)\n\n        print(f\"\\n{model_name} Metrics:\")\n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(f\"Precision: {precision:.4f}\")\n        print(f\"Recall: {recall:.4f}\")\n        print(f\"F1 Score: {f1:.4f}\")\n        print(f\"ROC-AUC: {roc_auc:.4f}\")\n        print(\"Confusion Matrix:\")\n        print(confusion_matrix(y_true, y_pred))\n        print(classification_report(y_true, y_pred))\n    \n    # Evaluate Logistic Regression\n    log_pred = log_model.predict(X_val_scaled)\n    print_metrics(y_val, log_pred, \"Logistic Regression\")\n    \n    # Evaluate Random Forest\n    rf_pred = rf_model.predict(X_val_scaled)\n    print_metrics(y_val, rf_pred, \"Random Forest\")\n    \n    # Evaluate XGBoost\n    xgb_pred = xgb_model.predict(X_val_scaled)\n    print_metrics(y_val, xgb_pred, \"XGBoost\")\n    \n    # Select the best model based on accuracy\n    best_model = max([(log_model, accuracy_score(y_val, log_pred)), \n                      (rf_model, accuracy_score(y_val, rf_pred)), \n                      (xgb_model, accuracy_score(y_val, xgb_pred))], key=lambda x: x[1])[0]\n    print(f\"\\nBest model selected: {best_model}\")\n    \n    return best_model\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T12:28:55.732195Z","iopub.execute_input":"2024-10-24T12:28:55.733106Z","iopub.status.idle":"2024-10-24T12:28:55.747484Z","shell.execute_reply.started":"2024-10-24T12:28:55.733066Z","shell.execute_reply":"2024-10-24T12:28:55.746534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 8: Make Predictions and Save to CSV","metadata":{"_uuid":"e3a5a288-271a-4f09-9bbe-b4fb7002cd38","_cell_guid":"bcdf31f2-1380-48a7-b0d7-255d606eb33b","trusted":true}},{"cell_type":"code","source":"def make_predictions_and_save_to_csv(model, processed_test, scaler, output_file='submission.csv'):\n    \"\"\"\n    Make predictions on the preprocessed test data and save the submission file as 'submission.csv'.\n    \n    Args:\n    model: Trained model to use for predictions\n    processed_test: Preprocessed test dataset\n    scaler: Fitted scaler object for scaling test features\n    output_file: Filename to save the final predictions (default: 'submission.csv')\n    \n    Returns:\n    None (saves CSV file)\n    \"\"\"\n    # Drop PassengerId from features\n    X_test = processed_test.drop(['PassengerId'], axis=1)\n    \n    # Scale the test data using the same scaler used for training data\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Make predictions on the scaled test data\n    predictions = model.predict(X_test_scaled)\n    \n    # Create a submission DataFrame\n    submission = pd.DataFrame({\n        'PassengerId': processed_test['PassengerId'],\n        'Survived': predictions\n    })\n    \n    # Save the submission file\n    submission.to_csv(output_file, index=False)\n    print(f\"Submission file saved as {output_file}\")\n\n# Usage Example:\n# make_predictions_and_save_to_csv(best_model, processed_test, scaler, 'submission.csv')","metadata":{"_uuid":"00f07816-7b2b-44ed-9fb4-35f7830be758","_cell_guid":"2db0c2f8-9b5f-42e7-b108-0800cd402933","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-24T12:28:55.750165Z","iopub.execute_input":"2024-10-24T12:28:55.750543Z","iopub.status.idle":"2024-10-24T12:28:55.764827Z","shell.execute_reply.started":"2024-10-24T12:28:55.750497Z","shell.execute_reply":"2024-10-24T12:28:55.763670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Main Function\n\nTo put everything together, hereâ€™s the final main function that runs the complete process.","metadata":{"_uuid":"984b4f29-2cbc-44a6-b0c1-2563ec0e1cf6","_cell_guid":"f191ceea-07f6-4a8f-bfe1-c4149a568dd9","trusted":true}},{"cell_type":"code","source":"def main():\n    # Step 1: Load and Explore Data\n    print(\"Step 1: Loading and exploring data...\")\n    df_train, df_test = load_data(\"/kaggle/input/titanic/train.csv\",\n                                  \"/kaggle/input/titanic/test.csv\")\n    \n    # Step 2: Preprocess the Data\n    print(\"Step 2: Preprocessing the data...\")\n    processed_train, processed_test = preprocess_data(df_train, df_test)\n    \n    # Step 3: Exploratory Data Analysis (EDA)\n    print(\"Step 3: Performing EDA...\")\n    create_key_visualizations(df_train)\n    \n    # Step 4: Split and Scale Data\n    print(\"Step 4: Splitting and scaling data...\")\n    X_train_scaled, X_val_scaled, y_train, y_val, scaler = split_and_scale_data(processed_train)\n    \n    # Step 5: Train and Tune Models\n    print(\"Step 5: Training and tuning models...\")\n    log_model = train_logistic_regression(X_train_scaled, y_train)\n    rf_model = tune_random_forest(X_train_scaled, y_train)\n    xgb_model = tune_xgboost(X_train_scaled, y_train)\n    \n    # Step 6: Plot Feature Importance (Random Forest)\n    print(\"Step 6: Plotting feature importance...\")\n    plot_feature_importance(rf_model, processed_train.drop(['Survived'], axis=1))\n    \n    # Step 7: Evaluate Models and Select the Best\n    print(\"Step 7: Evaluating models and selecting the best model...\")\n    best_model = evaluate_models(X_val_scaled, y_val, log_model, rf_model, xgb_model)\n    \n    # Step 8: Make Predictions and Save Submission File\n    print(\"Step 8: Making predictions and saving to CSV...\")\n    make_predictions_and_save_to_csv(best_model, processed_test, scaler, 'submission.csv')\n\n# Run the main function\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"4b7e7a54-52ed-48d8-8d2b-74dca6faf410","_cell_guid":"8f950060-4f09-4b9d-82a3-075e04b6589e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-24T12:28:55.767533Z","iopub.execute_input":"2024-10-24T12:28:55.767921Z","iopub.status.idle":"2024-10-24T12:31:38.123340Z","shell.execute_reply.started":"2024-10-24T12:28:55.767856Z","shell.execute_reply":"2024-10-24T12:31:38.122105Z"},"trusted":true},"execution_count":null,"outputs":[]}]}