---
title: "House Prices - Advanced Regression Techniques"
author: "Amir"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}
# rmarkdown Setting
knitr::opts_chunk$set(echo = TRUE)
```

## **Part 1: Project Description and Business Understanding**

### **1.1 Project Objective**

This project aims to predict residential home sale prices in Ames, Iowa, using 79 explanatory variables that describe home characteristics affecting property values. The dataset, sourced from the Ames Assessor's Office, is typically used for tax assessments but is also useful for home price modeling.

### **1.2 Potential Applications**

The predictive model has potential benefits for:

-   Home buyers to make more informed purchase decisions.
-   The Ames Assessor's Office to improve property tax assessments.

### **1.3 Target Audience**

-   **Home Buyers:** Gain insights into factors that influence home prices.
-   **Ames Assessor's Office:** Refine property valuations for more accurate tax assessments.

### **Required Libraries**

```{r Required Libraries}
# Load necessary libraries for data preprocessing, visualization, and modeling
library("ggplot2")      # Data visualization
library("moments")      # Statistical moments and tests (skewness, kurtosis)
library("MASS")         # Box-Cox transformations
library("visdat")       # Enhanced data visualization
library("corrplot")     # Correlation matrix visualization
library("caret")        # Machine learning tools
library("recipes")      # Feature engineering and preprocessing
library("car")          # Regression diagnostics
library("leaps")        # Regression subset selection
library("vip")          # Variable importance visualization
library("glmnet")       # Lasso and Elastic-Net regularized GLM
library("rpart")        # Decision trees for classification and regression
library("rpart.plot")   # Decision tree visualization
library("randomForest") # Random forests for regression and classification
library("gbm")          # Generalized Boosted Regression Models
library("xgboost")      # eXtreme Gradient Boosting
library("dplyr")        # Data manipulation
```

These libraries play key roles throughout the data science workflow, from preprocessing to model evaluation:

-   **Data Preprocessing and Exploration**: `dplyr`, `ggplot2`, `visdat` are used for cleaning, transforming, and visualizing data.
-   **Statistical Analysis and Transformations**: `moments`, `MASS`, `car` aid in statistical tests, data transformations, and regression diagnostics.
-   **Modeling and Evaluation**: `caret`, `glmnet`, `rpart`, `randomForest`, `leaps` provide tools for building and tuning models, including linear regression, regularization, decision trees, and random forests.
-   **Feature Engineering**: `recipes` supports feature creation and transformation.
-   **Model Interpretation**: `vip` helps interpret and visualize variable importance in models.

## **Part 2: Data Inspection**

### **2.1 Load Data From Source**

Kaggle's train and test datasets, containing house prices and related features, are loaded for analysis.

```{r read data from file}
# Set working directory
# setwd()

# Read main data from csv files
train <- read.csv("data/train.csv", header = TRUE, encoding="UTF-8")
test <- read.csv("data/test.csv", header = TRUE, encoding="UTF-8")
sample_submission <- read.csv("data/sample_submission.csv", header = TRUE, encoding="UTF-8")

# Dimension of data
dim(train)
dim(test)
```

### **Observation:**

-   The training dataset includes 81 features, with `SalePrice` as the target variable.
-   The test dataset has 80 features, excluding `SalePrice`, which is expected as it serves as the target variable.

### **2.3 Data Origin and Collection**

The Ames Housing dataset was developed by Dean De Cock for educational purposes in data science. Sourced from the Ames Assessor's Office, it includes property sales in Ames, Iowa, from 2006 to 2010. Initially, it featured 113 variables, later reduced to 81 variables relevant to property sales, categorized as nominal, ordinal, continuous, and discrete.

### **2.4 Variable Description**

For detailed variable descriptions, visit the [Kaggle link](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data).

```{r train Data Set Overview}
# Overview of the dataset
head(train)
tail(train)

# Get column names
colnames(train)

# Structure of the raw dataset
str(train)
```

### **2.5 Data Ambiguity and Measurement Errors**

-   **Ambiguities:** The data collection process lacks full detail, and some features marked as `NA` are not truly missing based on their descriptions.
-   **Possible Errors:** Given the data's origin from the Assessor's Office, questions remain about collection methods, such as whether they involved taxpayer self-reports or official assessments.

### **2.6 Potential Additional Variables**

Some variables that could enhance model accuracy were excluded due to complexity.

### **2.7 Variable Types**

The dataset contains 81 columns, including 23 nominal, 23 ordinal, 14 discrete, 20 continuous variables, and 1 identifier.

The training data consists of 1,460 instances, and the test data includes 1,459 instances, both sourced from Kaggle. Of the 81 attributes, 34 are quantitative, and 46 are qualitative (including `Id` and `SalePrice`).

In summary, the 81 attributes break down as follows: 46 categorical and 34 numerical, plus `Id` and `SalePrice`.

### **2.8 Handling Missing Values**

A preliminary check for missing values was conducted, focusing on specific cases in the training dataset.

```{r train data set - missing values}
# Summary of missing values
mv_train <- data.frame("variable_names" = colnames(train))
mv_train$mvs_freq <- apply(train, 2, function(x) sum(is.na(x)))
mv_train$mvs_percent <- round(mv_train$mvs_freq / nrow(train), 3) * 100

# Sort the summary by mvs_freq in descending order
mv_train <- mv_train[order(-mv_train$mvs_freq), ]

# (mv_train)
```

### **Missing Values in Test Dataset**

A similar check was conducted to identify missing values in the test dataset, ensuring consistency in data handling.

```{r test data set - missing values}

# Summary of missing values
mv_test <- data.frame("variable_names" = colnames(test))
mv_test$mvs_freq <- apply(test, 2, function(x) sum(is.na(x)))
mv_test$mvs_percent <- round(mv_test$mvs_freq / nrow(test), 3) * 100

# Sort the summary by mvs_freq in descending order
mv_test <- mv_test[order(-mv_test$mvs_freq), ]

# View(mv_test)
```

### **Conclusion**

Numerous features contain missing values. The first step is to eliminate features with excessive missing values to simplify the dataset. Next, a plan will be developed to appropriately handle the remaining missing values.

```{r temp - removing features with missing values}
train_non_mv <- dplyr::select(train, -LotFrontage, -Alley,
                              -MasVnrType, -MasVnrArea, -FireplaceQu,
                              -GarageType, -GarageYrBlt, -GarageFinish,
                              -GarageQual, -GarageCond, -BsmtQual, -BsmtCond,
                              -BsmtExposure, -BsmtFinType1, -PoolQC, -Fence,
                              -MiscFeature, -BsmtFinType1, -BsmtFinType2,
                              -Electrical, -BsmtFinSF2, -BsmtUnfSF,
                              -TotalBsmtSF, -BsmtFullBath, -BsmtHalfBath,
                              -GarageCars, -GarageArea, -MSZoning, -Utilities,
                              -Exterior1st, -Exterior2nd, -BsmtFinSF1,-KitchenQual,
                              -Functional,-SaleType)
dim(train_non_mv)
```

### **2.9 Statistical Summary**

#### **2.9.1 Univariate Profiling**

Statistical summaries and visualizations were generated for both categorical and continuous variables to understand their distributions and key characteristics.

```{r selecting numeric features}
# Selecting only numeric features
train_numerical <- select(train_non_mv, -HouseStyle, -RoofMatl, -Heating,
                          -Condition2, -RoofStyle, -ExterQual, -BldgType,
                          -ExterCond, -Foundation, -HeatingQC, -CentralAir,
                          -Condition1, -Neighborhood, -LandSlope, -LotConfig,
                          -LandContour, -LotShape, -Street, -PavedDrive,
                          -SaleCondition)
dim(train_numerical)
```

```{r selecting categorical features + SalePrice}
# Selecting only categorical features and the response value
train_categorical <- select(train_non_mv, HouseStyle, RoofMatl, Heating,
                            Condition2, RoofStyle, ExterQual, BldgType,
                            ExterCond, Foundation, HeatingQC, CentralAir,
                            Condition1, Neighborhood, LandSlope, LotConfig,
                            LandContour, LotShape, Street, PavedDrive,
                            SaleCondition, SalePrice)

dim(train_categorical)
```

```{r statistical summary of variables_1}
# Summary for categorical variables
summary(train_categorical[,1:20])

# Summary for numerical variables
summary(train_numerical)
```

```{r Id}
# check unique Id
dim(train)
length(unique(train$Id))
```

There is no redundant data according to Id

#### **2.9.2 Visualization**

Boxplots and histograms were used to visualize the distribution of continuous variables.

```{r continues variables - boxplot_1}
# # Boxplot of continuous variables
par(mar = c(2, 2, 2, 2))
par(mfrow = c(3, 4)) # 3 rows and 4 columns

for (i in seq(1:27)) {
  boxplot(train_numerical[, i],
          xlab = "",
          main = paste("Boxplot of ", colnames(train_numerical)[i])
          )
}

par(mfrow = c(1, 1))
```

```{r continuous variables - Histogram_1}
# Histogrtam of continuous variables
par(mar = c(2, 2, 2, 2))
par(mfrow = c(3, 4)) # 3 rows and 4 columns

for (i in seq(1:27)) {
  hist(train_numerical[, i],
       xlab = "",
       main = paste("Histogram of ",colnames(train_numerical)[i]))
  }

par(mfrow = c(1, 1))
```

Barplots were used to visualize the distribution of categorical variables.

```{r barplot of categorical variables_1}
# barplot of categorical variables
par(mar = c(2, 2, 2, 2))
par(mfrow = c(3, 4)) # 3 rows and 4 columns

for (i in seq(1:20)) {
  barplot(table(train_categorical[, i]),
          xlab = "",
          main = paste("Barplot of ", colnames(train_categorical)[i]))
  }

par(mfrow = c(1, 1))
```

#### **Results**

##### **1. Numerical Variables**

###### **Distribution**

-   Most numerical variables exhibit right-skewed distributions, with many smaller values and few larger ones.
-   Variables like "SalePrice", "LotArea", and "X1stFlrSF" show a concentration of smaller values, while skewness in "SalePrice" reflects a majority of affordable homes and a few high-priced outliers.

###### **Outliers**

-   Boxplots reveal potential outliers in variables such as "LotArea", "SalePrice", and "TotalBsmtSF". These may represent rare cases, such as larger properties, or possible data errors.

##### **2. Categorical Variables**

###### **Frequency Distributions**

-   Some categorical variables are dominated by a single category. - "RoofMatl" is primarily "CompShg" (98%). - "Heating" is mostly "GasA" (97%). - "Neighborhood" shows more diversity, though "NAmes" is the most frequent.

###### **Dominant Categories**

-   Binary variables like "CentralAir" are heavily skewed, with 93% of houses having central air conditioning. This highlights common traits in the dataset.

##### **3. Conclusion**

-   **Numerical Variables**: Skewness in key variables like "SalePrice" and the presence of outliers may affect model performance. Consider using log transformations or outlier handling to address these issues.

-   **Categorical Variables**: Several categorical features are dominated by a single category, which could limit their contribution to the model. However, variables like "Neighborhood" and "Condition1" show sufficient variability for predictive modeling.

-   Overall, the data displays skewness and category dominance, but variables with more diversity offer potential for robust modeling. Managing outliers and imbalances will be essential to improving model accuracy.

#### **2.9.3 Response Variable Analysis**

The target variable `SalePrice` was analyzed for its distribution, outliers, and normality.

```{r SalePrice}
# Summary statistics and histogram of SalePrice
summary(train$SalePrice)

# Missing Values
sum(is.na(train$SalePrice))
## good News :No missing values and zeros
## min = 34900, max = 755000

# Distribution of SalePrice
# histogram
hist(train$SalePrice, breaks = 20)
# Right skewed data

mean(train$SalePrice, na.rm = T)
median(train$SalePrice, na.rm = T)
## Right Skewed Data

# boxplot
boxplot(train$SalePrice)

## Quantiles for Sales Price
quantile(train$SalePrice)

# Tukey Method - outlier detection
# x > q(0.75) + 1.5 * IQR(x)
# x < q(0.25) - 1.5 * IQR(x)
low_outlier <- quantile(train$SalePrice, 0.25) - 1.5 * IQR(train$SalePrice)
# There is no low oulier based on tukey method and uni variate

high_outlier <- quantile(train$SalePrice, 0.75) + 1.5 * IQR(train$SalePrice)
sum(train$SalePrice > high_outlier)
high_outlier
# There are 61 high oulier based on tukey method and uni variate (SalePrice more than 340037.5 $$)
```

```{r Test of Normality Response Value }
# Test of Normality

# Histogram
hist(train$SalePrice, probability = T, breaks = 15)
lines(density(train$SalePrice), col = "red")

# QQ-plot
qqnorm(train$SalePrice, main = "QQ Plot of SalePrice", pch = 20)
qqline(train$SalePrice, col = "red")

# Test for Skewness and Kurtosis

# Jarque-Bera Test (H0: Skewness = 0 ?)
# p-value < 0.05 reject normality assumption
jarque.test(train$SalePrice)

# Anscombe-Glynn Test (H0: Kurtosis = 3 ?)
# p-value < 0.05 reject normality assumption
anscombe.test(train$SalePrice)

## Conclusion: reject normality assumption
```

**Conclusion:** The `SalePrice` distribution is right-skewed, and log transformation improves its normality.

**Performing Box-Cox Transformation**

```{r Box-Cox Transformation of SalePrice}
# transformed_x = (x ^ lambda - 1) /lambda if lambda <> 0
# transformed_x = log(x)                  if lambda = 0

box_results <- boxcox(train$SalePrice ~ 1, lambda = seq(-5, 5, 0.1))
box_results <- data.frame(box_results$x, box_results$y) # Create a data frame with the results
lambda <- 
  box_results[which(box_results$box_results.y == max(box_results$box_results.y)), 1]

lambda
```

lambda equal - 0.1, but confident interval include zero, so I can set lambda as 0 and choose log transformation for SalePrice based on Box-Cox Transformation.

```{r Transforming SalePrice}
# Rule of thumb :Transforming Skewed Data -> For right-skewed train: square root, cube root, and log

# Log transformation of SalePrice--------------------
log_SalePrice <- log(train$SalePrice)

# Histogram
hist(log_SalePrice, probability = T, breaks = 15)
lines(density(log_SalePrice), col = "red")

# QQ-plot
qqnorm(log_SalePrice, main = "QQ Plot", pch = 20)
qqline(log_SalePrice, col = "red")

# Test for Skewness and Kurtosis
# for sample size > 25

# Jarque-Bera Test (Skewness = 0 ?)
# p-value < 0.05 reject normality assumption
jarque.test(log_SalePrice)

# Anscombe-Glynn Test (Kurtosis = 3 ?)
# p-value < 0.05 reject normality assumption
anscombe.test(log_SalePrice)
```

**Conclusion:** The log transformation effectively reduces skewness.

select log transformation for sale price to close its distribution to normality. Using a log transformed Sale Price response means that errors in predicting expensive houses and cheap houses will affect the result equally.

Next we will apply a logarithmic transformation to make our distribution normal

```{r transform SalePrice}
train_non_mv$SalePrice <- log(train_non_mv$SalePrice)

train_numerical$SalePrice <- log(train_numerical$SalePrice)

train_categorical$SalePrice <- log(train_categorical$SalePrice)
```

### **2.10 Correlation Analysis**

Correlation analysis is performed to identify relationships between variables.

```{r check correlation for continues variables (correlation analysis)}
# Due to not normal distribution of numerical varaiables, I use Spearman method

# Correlation analysis between numeric variables except 'GarageYrBlt'
cor_table <- round(cor(train_numerical, method = "spearman"), 2)
cor_table
corrplot(cor_table)

```

### **2.11 Categorical vs. Numerical Variable Analysis**

Boxplots of categorical variables against `SalePrice` help identify potential predictors.

```{r categorical vs. numerical variables}
# boxplot
par(mar = c(2, 2, 2, 2))
par(mfrow = c(3, 4)) # 3 rows and 4 columns

for (i in seq(1:20)) {
  boxplot(train_categorical$SalePrice ~ train_categorical[, i],
          main = paste("SalePrice Vs. ", colnames(train_categorical)[i])
          )
}
par(mfrow = c(1, 1))
```

#### Correlation Analysis with SalePrice

##### Numerical Variables

The correlation analysis of numerical features with SalePrice reveals that certain features have strong positive relationships, such as OverallQual, GrLivArea, GarageCars, and GarageArea, all of which indicate the quality and size of the house. Other features like TotalBsmtSF and X1stFlrSF also show meaningful correlations, suggesting that the size of the basement and first floor are important in determining house prices.

On the other hand, variables like MiscVal, LowQualFinSF, and BsmtHalfBath exhibit very low or negative correlations with SalePrice, indicating they contribute little to predicting house prices. MoSold and YrSold, which represent the month and year of sale, show weak correlations, and unless seasonality is important, they can likely be excluded.

##### Categorical Variables (Encoded)

Among the encoded categorical variables, ExterQual_Gd shows a notably strong correlation with SalePrice, emphasizing the importance of exterior quality in predicting house prices. BldgType_1Fam and RoofStyle_Gable also have positive correlations, suggesting that the type of building and roof style play a role in house valuation.

However, features such as ExterCond_TA, BldgType_Twnhs, and BldgType_2fmCon have weak or negative correlations with SalePrice, indicating they have little influence on price prediction and may be excluded from the model.

##### **Conclusion**

The strongest correlations in both numerical and categorical variables suggest that features related to house quality, size, and exterior appearance are the most important for predicting SalePrice. Features with low correlation or zero variance, such as MiscVal, LowQualFinSF, BsmtHalfBath, MoSold, YrSold, ExterCond_TA, BldgType_Twnhs, and BldgType_2fmCon, could be removed to streamline the model and improve performance.

By focusing on the features with stronger correlations, the model's predictive power can be improved, while removing features with low or no relevance will reduce noise and complexity.

```{r removing features}
# removing low correlation numerical
mdf_train <- select(train_non_mv, -MSSubClass, -OverallCond, -LowQualFinSF,
                    -BedroomAbvGr, -KitchenAbvGr, -EnclosedPorch, -X3SsnPorch,
                    -ScreenPorch, -PoolArea, -MiscVal, -MoSold, -YrSold)
# removing low variance categorical
mdf_train <- select(mdf_train, -RoofStyle, -BldgType, -LandSlope, -LotConfig,
                    -LandContour, -Heating, -ExterCond, -RoofMatl, -Condition1,
                    -Condition2, -Street)
```

### **2.12 Comparison of Raw and Modified Data**

The `arsenal::comparedf` function is used to compare the raw and processed datasets, ensuring that all necessary transformations have been applied correctly.

```{r check changes of raw data vs. modified data}
arsenal::comparedf(train, mdf_train)
```

## **Part 3: Data Preparation**

### **3.1 Handling Missing Values**

We will eliminate the features with a high amount of missing data (more than 15%) as inferring values would likely require too much effort and could introduce bias to the training. For the remaining features with low missing count, we will use a simple technique to fill in the missing data by replacing them with the median or mode of the feature.

```{r processing features with missing values}
train_mv_numerical <- select(train, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF,
                             BsmtFullBath, BsmtHalfBath, GarageCars, 
                             GarageArea, BsmtFinSF1)

train_mv_categorical <- select(train, MasVnrArea, GarageYrBlt, MasVnrType,
                               GarageType, GarageFinish, GarageQual, GarageCond,
                               BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1,
                               BsmtFinType1, BsmtFinType2, Electrical, MSZoning,
                               Utilities, Exterior1st, Exterior2nd, KitchenQual,
                               Functional, Fence, Alley, PoolQC, MiscFeature,
                               SaleType)
```

```{r impute missing values}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

for(i in 1:ncol(train_mv_numerical)){
  train_mv_numerical[is.na(train_mv_numerical[,i]), i] <- mean(train_mv_numerical[,i], na.rm = TRUE)
}

for(i in 1:ncol(train_mv_categorical)){
  train_mv_categorical[is.na(train_mv_categorical[,i]), i] <- Mode(train_mv_categorical[,i])
}
```

### correlation of numerical features with missing value

Next, let's observe how the recently generated date interacts with the target feature, using the same approach as previously.

We'll start with the numerical features.

```{r }
# add target feature to the set
train_mv_numerical <- cbind(train_mv_numerical, select(train, SalePrice))

# Store the overall correlation in 'correlations'
cor_table <- round(cor(train_mv_numerical, method = "spearman"), 2)
cor_table
# Plot the correlation plot with 'correlations'
corrplot(cor_table)

```

Then the categorical features.

```{r }
train_mv_categorical <- cbind(train_mv_categorical, select(train, SalePrice))
# boxplot
par(mar = c(2, 2, 2, 2))
par(mfrow = c(3, 4)) # 3 rows and 4 columns

for (i in seq(1:24)) {
  boxplot(train_mv_categorical$SalePrice ~ train_mv_categorical[, i],
    main = paste("SalePrice Vs. ", colnames(train_mv_categorical)[i])
  )
}
par(mfrow = c(1, 1))
```

As you can see we still have a number of irrelevant features, so we will also remove them.

```{r }
mdf_train_mv <- cbind(select(train_mv_categorical,
                             -SalePrice),
                      train_mv_numerical)

modified_train <- cbind(select(mdf_train_mv, -Alley, -BsmtFinType2, -Fence,
                               -Functional, -Utilities, -PoolQC, -BsmtHalfBath,
                               -BsmtFinSF2, -MiscFeature,- GarageYrBlt, -SalePrice),
                        mdf_train)
```

Then we will do the same process to our test set.

```{r procees test data set}
test_non_mv <- select(test, -LotFrontage, -Alley, -MasVnrType, -MasVnrArea,
                      -FireplaceQu, -GarageType, -GarageYrBlt, -GarageFinish,
                      -GarageQual, -GarageCond, -BsmtQual, -BsmtCond, -BsmtExposure,
                      -BsmtFinType1, -PoolQC, -Fence, -MiscFeature, -BsmtFinType1,
                      -BsmtFinType2, -Electrical, -BsmtFinSF2, -BsmtUnfSF,
                      -TotalBsmtSF, -BsmtFullBath, -BsmtHalfBath, -GarageCars,
                      -GarageArea, -MSZoning, -Utilities, -Exterior1st,
                      -Exterior2nd,  -BsmtFinSF1, -KitchenQual, -Functional,
                      -SaleType)

mdf_test <- select(test_non_mv, -MSSubClass, -OverallCond, -LowQualFinSF,
                   -BedroomAbvGr, -KitchenAbvGr, -EnclosedPorch, -X3SsnPorch,
                   -ScreenPorch, -PoolArea, -MiscVal, -MoSold, -YrSold)

mdf_test <- select(mdf_test, -RoofStyle, -BldgType, -LandSlope, -LotConfig,
                   -LandContour, -Heating, -ExterCond, -RoofMatl, -Condition1,
                   -Condition2, -Street)

test_mv_numerical <- select(test, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath,
                            BsmtHalfBath, GarageCars, GarageArea, BsmtFinSF1)


test_mv_categorical <- select(test, MasVnrArea, GarageYrBlt, MasVnrType, GarageType,
                              GarageFinish, GarageQual, GarageCond, BsmtQual,
                              BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType1,
                              BsmtFinType2, Electrical, MSZoning, Utilities,
                              Exterior1st, Exterior2nd, KitchenQual, Functional,
                              Fence, Alley, PoolQC, MiscFeature, SaleType)



for(i in 1:ncol(test_mv_numerical)){
  test_mv_numerical[is.na(test_mv_numerical[,i]), i]<- mean(test_mv_numerical[,i], na.rm = TRUE)
  }

for(i in 1:ncol(test_mv_categorical)){
  test_mv_categorical[is.na(test_mv_categorical[,i]), i] <- Mode(test_mv_categorical[,i])
  }

mdf_test_mv <- cbind(test_mv_categorical, test_mv_numerical)

modified_test <- cbind(select(mdf_test_mv, -Alley, -BsmtFinType2, -Fence,
                              -Functional, -Utilities, -PoolQC, -BsmtHalfBath,
                              -BsmtFinSF2, -MiscFeature, -GarageYrBlt), mdf_test)
```

```{r}
dim(modified_test)
dim(modified_train)
```


### **3.3.2 Outlier Handling**

Outliers in `SalePrice` and other key features are identified . In general, for models like Random Forest and XGBoost, you do not necessarily need to remove outliers, as these algorithms are robust to outliers..
### **3.3.3 Feature Engineering**

We further refine the dataset by creating new features based on domain knowledge.

```{r Reprocess train set}
# Creating some new features based on domain knowledge and previous analysis

# TotalSF
modified_train$TotalSF <- modified_train$TotalBsmtSF + modified_train$GrLivArea

# Total_Bathrooms
modified_train$Total_Bathrooms <- modified_train$FullBath + (0.5 * modified_train$HalfBath) + modified_train$BsmtFullBath

# TotalPorchSF
modified_train$TotalPorchSF <- modified_train$OpenPorchSF + modified_train$WoodDeckSF

# Correlation check for new features
new_features <- c("TotalSF", "Total_Bathrooms", "TotalPorchSF")
correlation_with_saleprice <- cor(modified_train[c(new_features, "SalePrice")])[, "SalePrice"]
correlation_with_saleprice <- sort(correlation_with_saleprice, decreasing = TRUE)

correlation_with_saleprice

```

**Conclusion:**\
We dropped extra numerical features that introduced multicollinearity and retained those with the highest correlation with `SalePrice`.

```{r Drop features with high multicollinearity}
# Drop features with high multicollinearity
modified_train <- select(modified_train, -TotalBsmtSF, -FullBath, -HalfBath,
                         -BsmtFullBath, -OpenPorchSF, -WoodDeckSF)
  
dim(modified_train)
```

modify test set 
```{r Reprocess test set}
# Creating some new features based on domain knowledge and previous analysis

# TotalSF
modified_test$TotalSF <- modified_test$TotalBsmtSF + modified_test$GrLivArea

# Total_Bathrooms
modified_test$Total_Bathrooms <- modified_test$FullBath + (0.5 * modified_test$HalfBath) + modified_test$BsmtFullBath

# TotalPorchSF
modified_test$TotalPorchSF <- modified_test$OpenPorchSF + modified_test$WoodDeckSF

```

We dropped extra numerical features.

```{r Drop features with high multicollinearity}
# Drop features with high multicollinearity
modified_test <- select(modified_test, -TotalBsmtSF, -FullBath, -HalfBath,
                         -BsmtFullBath, -OpenPorchSF, -WoodDeckSF)
  
dim(modified_test)
```


### **3.4 Data Set Preparation for Modeling**

#### Prepare data set for train , validation and test

steps for creating Train and Validation set:
Steps for creating data_2:

1- Filter out near-zero variance features .

2- Collapsing factor levels with threshold 0.5

3 -Ordinarily encode all quality features, which are on a 1–10 Likert scale.

4- Standardize (center and scale) all numeric features.

```{r Divide Data Set into Train and Test}
# Data 1: Filter out near-zero variance features, collapse low-frequency categories
# Divide Data set into Train and Test--------------
set.seed(42)
train_cases <- sample(1:nrow(modified_train), nrow(modified_train) * 0.7)
train_1 <- modified_train[train_cases, ]
test_1 <- modified_train[-train_cases, ]
 
blueprint_1 <- recipe(SalePrice ~ ., data = train_1) %>%
  step_rm("Id") %>%
  step_nzv(all_nominal()) %>%
  step_nzv(all_numeric()) %>%
  step_other(matches("Electrical|RoofStyle|SaleType|Neighborhood"),
             threshold = 0.05, other = "Other") %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes())


blueprint_1

# we do not want to train on the test data ,as this would create data leakage. So in this step we estimate these parameters based on the training data of interest.
prepare_1 <- prep(blueprint_1, training = train_1)
prepare_1

# Lastly, we can apply our blueprint to new data (e.g., the training data or future test data)
train_1 <- bake(prepare_1, new_data = train_1)

test_1 <- bake(prepare_1, new_data = test_1)

test_kaggle <- bake(prepare_1, new_data = modified_test)

dim(train_1)
summary(train_1)
dim(test_1)
summary(test_1)

# check random selection of train and test data
summary(train_1$SalePrice)
summary(test_1$SalePrice)

summary(train_1$TotalSF)
summary(test_1$TotalSF)
```

### **3.5 Memory Management**

Finally, I release memory by removing temporary variables to optimize performance.

```{r Release Memory Part_2}
# Remove all unused variables 
rm(list = setdiff(ls(), 
                  c("modified_train", "modified_test", 
                    "train_1", "test_1", 
                    "train", "test", "test_kaggle"
                    )
                  )
   )

# Clear plot devices to release memory
dev.off()
gc()
```

## **Part 4: Predictive Models**

### **Part 4.1: Linear Regression**

We begin by developing linear regression models using different datasets. We evaluate the assumptions of linear regression, diagnose potential issues, and refine the models accordingly.

#### **Model 1 with Data 1**

```{r Model lm_1}
# Model 1: Initial Linear Regression

lm_1 <- lm(SalePrice ~ . , data = train_1)
options(max.print = 999999)
summary(lm_1)

# Check Assumptions of Regression

# Normality of residuals
hist(lm_1$residuals, probability = TRUE)
lines(density(lm_1$residuals), col = "red")

# QQ-plot
qqnorm(lm_1$residuals, main = "QQ Plot of residuals lm_1", pch = 20)
qqline(lm_1$residuals, col = "red")

# Test for Skewness and Kurtosis
# Good for sample size > 25
# Jarque-Bera Test (Skewness = 0 ?)
# p-value < 0.05 reject normality assumption
jarque.test(lm_1$residuals)

# Anscombe-Glynn Test (Kurtosis = 3 ?)
# p-value < 0.05 reject normality assumption
anscombe.test(lm_1$residuals)

# Note: Residuals are not Normally Distributed!

# Diagnostic Plots
plot(lm_1)

# Cook's distance > 1
head(cooks.distance(lm_1))
sum(cooks.distance(lm_1) > 1, na.rm = T)

# Check multicollinearity
#car :: vif(lm_1)
# The Reason of Error is : High Multicollinearity

# Note: Residuals are not Normally Distributed!
# Note: low Hetroscedasticity Problem!

# Conclusion:
# severe violation of regression assumptions
# Bad model!
```

**Conclusion:**\
The initial linear regression model exhibits severe violations of regression assumptions, including non-normal residuals and high multicollinearity. This model is considered inadequate for prediction.

#### **Model 1_t with Data 1 (Refined)**

Find best features based on t-test vip method

```{r Very import features of Model_1}
vip(lm_1, num_features = 40, geom = "point")
```

Factors might affect the value of home price, including the neighborhood it is located in, the size of its lot and the age and condition of the structure itself.

Most important factors for estimating a home price:

1- Size: total area of a house (basement area + first and second floor area)

2- Neighborhood

3- quality

4- Facilities

Define a Model based on t-test results and the most important factors of home price:

```{r t_test_1 subset_formula}
# Define a formula for estimating a home price
subset_formula <- as.formula(SalePrice ~
  # Size of Property
  TotalSF + Total_Bathrooms +
  LotArea + TotalPorchSF +
  # Year
  YearBuilt + YearRemodAdd +
  # Neighborhood & SaleType
  SaleCondition + MSZoning +
  # Conditon and Quality
  OverallQual +
  # additional facilities
  GarageCars  - SalePrice )
```

```{r Model_1 based on t-test results}
# Define a refined model based on t-test results and important factors
lm_1_t <- lm(subset_formula, data = train_1)
summary(lm_1_t)
car::vif(lm_1_t)


# Check Assumptions of Regression -------------
# Normality of residuals
hist(lm_1_t$residuals, probability = TRUE)
lines(density(lm_1_t$residuals), col = "red")

# QQ-plot
qqnorm(lm_1_t$residuals, main = "QQ Plot of residuals lm_1", pch = 20)
qqline(lm_1_t$residuals, col = "red")

# Test for Skewness and Kurtosis
# Good for sample size > 25
# Jarque-Bera Test (Skewness = 0 ?)
# p-value < 0.05 reject normality assumption
jarque.test(lm_1_t$residuals)

# Anscombe-Glynn Test (Kurtosis = 3 ?)
# p-value < 0.05 reject normality assumption
anscombe.test(lm_1_t$residuals)

# Note: Residuals are not Normally Distributed!

# Diagnostic Plots
plot(lm_1_t)

# Note: low Hetroscedasticity Problem!

# Cook's distance > 1
head(cooks.distance(lm_1_t))
sum(cooks.distance(lm_1_t) > 1) # ,na.rm = TRUE)

# Check multicollinearity
car::vif(lm_1_t)
# No Multicollinearity
```

**Conclusion:**\
This refined model shows improvement but still has some residual issues. We proceed by removing influential points identified by Cook's distance.

```{r Model_1 based on t-test results and remove bad cases}
# Remove influential cases and refit the model
train_1_1 <- train_1[-which(rownames(train_1) %in% c( 33, 203, 234, 236, 291,
                                                      354,722, 782 , 424,
                                                      426, 718, 937,1021)), ] 
# in third try

# Build new model wity modified train_1
lm_1_t <- lm(subset_formula, data = train_1_1)

summary(lm_1_t)


# Check Assumptions of Regression -------------
# Normality of residuals
hist(lm_1_t$residuals, probability = TRUE)
lines(density(lm_1_t$residuals), col = "red")

# QQ-plot
qqnorm(lm_1_t$residuals, main = "QQ Plot of residuals lm_1", pch = 20)
qqline(lm_1_t$residuals, col = "red")

# Test for Skewness and Kurtosis
# Good for sample size > 25
# Jarque-Bera Test (Skewness = 0 ?)
# p-value < 0.05 reject normality assumption
jarque.test(lm_1_t$residuals)

# Anscombe-Glynn Test (Kurtosis = 3 ?)
# p-value < 0.05 reject normality assumption
anscombe.test(lm_1_t$residuals)

# Note: Residuals are not Normally Distributed!
# Alleviate the problem by removing some instances

# Diagnostic Plots
plot(lm_1_t)
# Note: low Hetroscedasticity Problem!
# Alleviate the problem by removing bad cases

# Cook's distance > 1
head(cooks.distance(lm_1_t))
sum(cooks.distance(lm_1_t) > 1) # ,na.rm = TRUE)

# Check multicollinearity
car::vif(lm_1_t)
# No Multicollinearity
```

**Model Testing:**

```{r Model Testing:}
# Test the Model----------------------------------
# Model: lm_2
# Prediction
pred_lm_1_t <- predict(lm_1_t, test_1)
pred_lm_1_t <- exp(pred_lm_1_t)
# colnames(pred_lm_1_t) <- "Pred_SalePrice"
summary(pred_lm_1_t)
# Absolute error mean, median, sd, max, min-------
abs_err_lm_1_t <- abs(pred_lm_1_t - exp(test_1$SalePrice))

# Create a dataframe to save prediction results on test data set
models_comp <- data.frame(
  "Mean_of_AbsErrors" = mean(abs_err_lm_1_t),
  "Median_of_AbsErrors" = median(abs_err_lm_1_t),
  "SD_of_AbsErrors" = sd(abs_err_lm_1_t),
  "IQR_of_AbsErrors" = IQR(abs_err_lm_1_t),
  "Min_of_AbsErrors" = min(abs_err_lm_1_t),
  "Max_of_AbsErrors" = max(abs_err_lm_1_t),
  row.names = "LM_t-test_1"
)
View(models_comp)
models_comp
# Actual vs. Predicted
plot(exp(test_1$SalePrice), pred_lm_1_t,
  main = "LM_t-test_1",
  xlab = "Actual", ylab = "Prediction"
)
abline(a = 0, b = 1, col = "red", lwd = 2)
```

### **Part 4.2: Principal Component Regression (PCR)**

In this section, we employ Principal Component Regression to reduce dimensionality and improve model performance.

#### **Model 1 with Data 1**

```{r Required Library_2}
library("pls") # Principal Component Regression
```

### Model_1 with data_1

```{r Building Prediction Model_1 - PCR - data_1}
# Build PCR model
set.seed(42)
pcr_res_1 <- pcr(SalePrice ~ . ,
  data = train_1,
  validation = "CV"
)

summary(pcr_res_1)

# pcr_res_1$coefficients
# pcr_res_1$scores
score_mat_1 <- as.matrix(pcr_res_1$scores)

# Select best number of components
validationplot(pcr_res_1, val.type = "RMSEP")
pls_RMSEP_1 <- RMSEP(pcr_res_1, estimate = "CV")
pls_RMSEP_1
min_comp_1 <- which.min(pls_RMSEP_1$val) - 1
min_comp_1
points(min_comp_1, min(pls_RMSEP_1$val), pch = 1, col = "red")

## Model Testing
pred_pcr_1 <- predict(pcr_res_1, test_1, ncomp = min_comp_1)
pred_pcr_1 <- exp(pred_pcr_1)

summary(pred_pcr_1)

abs_err_pcr_1 <- abs(pred_pcr_1 - exp(test_1$SalePrice))

# Create a dataframe to save prediction results on test data set
models_comp <- rbind(
  models_comp,
  data.frame(
    "Mean_of_AbsErrors" = mean(abs_err_pcr_1, na.rm = TRUE),
    "Median_of_AbsErrors" = median(abs_err_pcr_1, na.rm = TRUE),
    "SD_of_AbsErrors" = sd(abs_err_pcr_1, na.rm = TRUE),
    "IQR_of_AbsErrors" = IQR(abs_err_pcr_1, na.rm = TRUE),
    "Min_of_AbsErrors" = min(abs_err_pcr_1, na.rm = TRUE),
    "Max_of_AbsErrors" = max(abs_err_pcr_1, na.rm = TRUE),
    row.names = "PCR_1"
  )
)
View(models_comp)
models_comp

plot(exp(test_1$SalePrice), pred_pcr_1,
  main = "PCR Model_1",
  xlab = "Actual", ylab = "Prediction"
)
abline(a = 0, b = 1, col = "red", lwd = 2)
```

## **Part 4.4: Decision Trees and Random Forest**

In this section, we develop decision trees and random forests to model the data, examining the structure and importance of variables.

#### **Model 1: Decision Tree with Data 1**

```{r Model 1: Decision Tree Model Using All features}
# Decision Tree

# Decision Tree Model Using All Variables
tree_1 <- rpart(
  formula = SalePrice ~ . ,
  data = train_1,
  control = list(
    cp = 0.0001,
    maxdepth = 3,
    minbucket = 15
  )
)
# Plot the tree
prp(tree_1)

# Prune the tree
plotcp(tree_1)
tree_1$cptable[which.min(tree_1$cptable[, "xerror"])]

# Prune the tree
tree_1_P <- prune.rpart(tree_1,
  cp = tree_1$cptable[which.min(tree_1$cptable[, "xerror"])]
)

# Plot the tree
prp(tree_1_P)

# Test the Model----------------------------------
# Prediction: tree_1
pred_tree_1 <- predict(tree_1_P, test_1)
pred_tree_1 <- exp(pred_tree_1)
summary(pred_tree_1)

# Absolute error mean, median, sd, max, min-------
abs_err_tree_1 <- abs(pred_tree_1 - exp(test_1$SalePrice))
models_comp <- rbind(models_comp, "TreeReg_1" = c(
  mean(abs_err_tree_1),
  median(abs_err_tree_1),
  sd(abs_err_tree_1),
  IQR(abs_err_tree_1),
  range(abs_err_tree_1)
))

View(models_comp)
models_comp
# Actual vs. Predicted
plot(exp(test_1$SalePrice), pred_tree_1,
  main = "TreeReg_1",
  xlab = "Actual", ylab = "Prediction"
)
abline(a = 0, b = 1, col = "red", lwd = 2)
```


#### **Model 1: Bagging with data_1**

```{r Model 1: Bagging}
# Bagging

set.seed(724)
bagging_1 <- randomForest(SalePrice ~ . ,
  mtry = 38,
  ntree = 1000,
  data = train_1
)

bagging_1

# Model Testing

# Prediction: Bagging
pred_bagging_1 <- predict(bagging_1, test_1)
pred_bagging_1 <- exp(pred_bagging_1)
summary(pred_bagging_1)

# Absolute error mean, median, sd, max, min-------
abs_err_bagging_1 <- abs(pred_bagging_1 - exp(test_1$SalePrice))
models_comp <- rbind(models_comp, "Bagging_1" = c(
  mean(abs_err_bagging_1, na.rm = TRUE),
  median(abs_err_bagging_1, na.rm = TRUE),
  sd(abs_err_bagging_1, na.rm = TRUE),
  IQR(abs_err_bagging_1, na.rm = TRUE),
  range(abs_err_bagging_1, na.rm = TRUE)
))

View(models_comp)
models_comp
# Actual vs. Predicted
plot(exp(test_1$SalePrice), pred_bagging_1,
  main = "Bagging_1",
  xlab = "Actual", ylab = "Prediction"
)
abline(a = 0, b = 1, col = "red", lwd = 2)
```

#### **Model 1: Random Forest with Data 1**

```{r Model 1: Random Forest}
# Random Forest

set.seed(724)

# mtry	for regression = p/3  ---> 14
rf_1 <- randomForest(SalePrice ~ . ,
  data = train_1,
  mtry = 14, ntree = 1000,
  nodesize = 10, importance = TRUE
)

rf_1

importance(rf_1)
varImpPlot(rf_1)

# # Cross-Validation and Feature Selection
## step = 0.9,
## mtry default: floor(sqrt(p)), floor(p/3)
# recursive: whether variable importance is (re-)assessed at each step of variable reduction

#  remove "SalePrice"[53 ] & "log_SalePrice"[54]
colnames(train_1[, c(39)])

set.seed(724)
rf_cv_1 <- rfcv(train_1[, -c(39)],
                train_1$SalePrice,
                cv.fold = 10,
                step = 0.9,
                mtry = function(p) max(1, floor(sqrt(p))),
                recursive = FALSE
                )

class(rf_cv_1)
str(rf_cv_1)

# Vector of number of variables used at each step
rf_cv_1$n.var

# Corresponding vector of MSEs at each step
rf_cv_1$error.cv
which.min(rf_cv_1$error.cv)

# select 22 variables based on Importance of Variables
sort(importance(rf_1, )[, 1])

# Regression formula
reg_formula_1 <- as.formula(SalePrice ~  MSZoning  + Exterior1st + 
                              CentralAir + BsmtFinType1 +
                              X2ndFlrSF + MSZoning + GarageCars + 
                              Total_Bathrooms + YearRemodAdd + X1stFlrSF +
                              GarageArea + BsmtFinSF1 + LotArea + 
                              YearBuilt + Neighborhood + GrLivArea +
                              OverallQual + TotalSF )


reg_formula_1
class(reg_formula_1)

# Tuning Hyperparameters
# Set random seed
set.seed(724)

# Define a function to perform random forest training with different hyperparameters
train_rf <- function(data, formula, mtry, ntree, nodesize) {
# Train random forest model
  model <- randomForest(formula, data = data, mtry = mtry, ntree = ntree, nodesize = nodesize, importance = TRUE)
  
  # Return the model and importance scores
  return(list(model = model, importance = importance(model)))
}

# Define hyperparameter grid
mtry_grid <- c(floor(sqrt(ncol(train_1))), floor(ncol(train_1)/3))
ntree_grid <- c(500, 1000, 1500, 2000)

# Initialize empty list to store results
results <- list()

# Loop through each combination of mtry and ntree
for (i in 1:length(mtry_grid)) {
  for (j in 1:length(ntree_grid)) {
    # Current hyperparameter combination
    current_mtry <- mtry_grid[i]
    current_ntree <- ntree_grid[j]
    
    # Train model with current hyperparameters
    model_result <- train_rf(train_1, reg_formula_1, current_mtry, current_ntree, nodesize = 7)
    
    # Store results in a list
    results[[paste("mtry_", current_mtry, "_ntree_", current_ntree, sep = "")]] <- model_result
  }
}

# Function to evaluate model performance (replace with your preferred metric)
evaluate_model <- function(model) {
  # Example: Calculate mean squared error (MSE) on test data
  predictions <- predict(model$model, test_1)
  Rmse <- sqrt(mean((predictions - test_1$SalePrice)^2))
  return(Rmse)
}

# Evaluate each model based on chosen metric
model_evaluations <- sapply(results, function(x) evaluate_model(x))

# Find the best model based on minimum evaluation score
best_model_index <- which.min(model_evaluations)
best_model_name <- names(model_evaluations)[best_model_index]

# Extract the best model and importance scores
best_model <- results[[best_model_name]]$model
best_importance <- results[[best_model_name]]$importance

# Use the best model for prediction
pred_rf_1 <- predict(best_model, test_1)
pred_rf_1 <- exp(pred_rf_1)

summary(pred_rf_1)
# Absolute error mean, median, sd, max, min-------
abs_err_rf_1 <- abs(pred_rf_1 - exp(test_1$SalePrice))
models_comp <- rbind(models_comp, "RandomForest_1" = c(
  mean(abs_err_rf_1),
  median(abs_err_rf_1),
  sd(abs_err_rf_1),
  IQR(abs_err_rf_1),
  range(abs_err_rf_1)
))

View(models_comp)
models_comp

# Actual vs. Predicted
plot(exp(test_1$SalePrice), pred_rf_1,
  main = "RandomForest_1",
  xlab = "Actual", ylab = "Prediction"
)
abline(a = 0, b = 1, col = "red", lwd = 2)
```

#### Test kaggle test data set

```{r}
# Use the best model for prediction
pred_rf_k_1 <- predict(best_model, test_kaggle)
pred_rf_k_1 <- exp(pred_rf_k_1)

sum(is.na(pred_rf_k_1))

pred_rf_k_1 <- cbind("Id" = test$Id, "SalePrice" = pred_rf_k_1)

write.csv(pred_rf_k_1, file = "results/pred_rf_k_1.csv", row.names = FALSE)

#Save the results
save(best_model, file = "results/RandomForest_1.R")
```

### **Boosting Methods in Regression**

### **Model 1: Gradient Boosting (GB) Regression with Data 1**

Gradient Boosting is a powerful ensemble method that builds models sequentially, each new model correcting errors made by the previous ones. We begin with two GBM models using different parameters and then proceed to hyperparameter tuning for optimal performance.

#### **Initial GBM Model**

```{r GB Regression}
# Train GBM model with basic parameters
set.seed(724)

gbm_1 <- gbm(formula = SalePrice ~ . ,
             distribution = "gaussian", #Gaussian for regression problems
             data = train_1,
             n.trees = 2000, #The total number of trees to fit
             interaction.depth = 1, #1: stump, the maximum depth of each tree 
             shrinkage = 0.01, #Learning rate
             cv.folds = 5,   #Number of cross-validation folds to perform
             n.cores = NULL, #Use all cores by default
             verbose = FALSE)  

#Get MSE and compute RMSE
min(gbm_1$cv.error)         #MSE
sqrt(min(gbm_1$cv.error))   #RMSE

#plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm_1, method = "cv")
which(gbm_1$cv.error == min(gbm_1$cv.error))
#returns the estimated optimal number of iterations
```

#### **Second GBM Model with Different Parameters**

```{r}
# Train GBM model with different parameters
set.seed(724)
gbm_2 <- gbm(formula = SalePrice ~ . ,
             distribution = "gaussian",
             data = train_1,
             n.trees = 200,
             interaction.depth = 5,
             shrinkage = 0.1,
             cv.folds = 5,
             n.cores = NULL, #will use all cores by default
             verbose = FALSE)  

#get MSE and compute RMSE
min(gbm_2$cv.error)
sqrt(min(gbm_2$cv.error))

#plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm_2, method = "cv")
which(gbm_2$cv.error == min(gbm_2$cv.error))
```

#### **Hyperparameter Tuning**

To find the optimal set of hyperparameters, we perform a grid search over a range of values for learning rate, interaction depth, minimum observations in nodes, and bag fraction.

```{r}
#Tuning
#Create hyper-parameter grid
par_grid <- expand.grid(shrinkage = c(0.01, 0.1, 0.3),  #learning rate
                        interaction_depth = c(5, 10, 15), #the maximum depth of each tree
                        n_minobsinnode = c(5, 10, 15),  #the minimum number of observations in the terminal nodes of the trees
                        bag_fraction = c(0.5, 0.7, 0.9) #stochastic gradient :bag.fraction < 1
)
# view(par_grid)
nrow(par_grid)

#Grid search (train/validation approach)
for(i in 1 : nrow(par_grid) ) {
  set.seed(724)
  #train model
  gbm_tune <- gbm(formula = SalePrice ~ . ,
                  distribution = "gaussian",
                  data = train_1,
                  n.trees = 2000,
                  interaction.depth = par_grid$interaction_depth[i],
                  shrinkage = par_grid$shrinkage[i],
                  n.minobsinnode = par_grid$n_minobsinnode[i],
                  bag.fraction = par_grid$bag_fraction[i],
                  train.fraction = 0.8,
                  cv.folds = 0,
                  n.cores = NULL, #will use all cores by default
                  verbose = FALSE)  
  #add min training error and trees to grid
  par_grid$optimal_trees[i] <- which.min(gbm_tune$valid.error)
  par_grid$min_RMSE[i]      <- sqrt(min(gbm_tune$valid.error))
}

head(par_grid)
View(par_grid)
par_grid_1 <- par_grid
#Modify hyper-parameter grid
par_grid <- expand.grid(shrinkage = c(0.01, 0.05, .1),
                        interaction_depth = c(3, 4, 5),
                        n_minobsinnode = c(5, 7, 10),
                        bag_fraction = c(0.5, 0.7, 0.9)  #stochastic gradient :bag.fraction < 1
)

nrow(par_grid)
#Grid search (train/validation approach)
for(i in 1 : nrow(par_grid)) {
  set.seed(123)
  #train model
  gbm_tune <- gbm(formula = SalePrice ~ . ,
                  distribution = "gaussian",
                  data = train_1,
                  n.trees = 2000,
                  interaction.depth = par_grid$interaction_depth[i],
                  shrinkage = par_grid$shrinkage[i],
                  n.minobsinnode = par_grid$n_minobsinnode[i],
                  bag.fraction = par_grid$bag_fraction[i],
                  train.fraction = 0.8,
                  cv.folds = 0,
                  n.cores = NULL, #will use all cores by default
                  verbose = FALSE)  
  #add min training error and trees to grid
  par_grid$optimal_trees[i] <- which.min(gbm_tune$valid.error)
  par_grid$min_RMSE[i]    <- sqrt(min(gbm_tune$valid.error))
}

save(par_grid, file = "results/par_grid_GBM.R") #save results
load("results/par_grid_GBM.R")                  #load results

head(par_grid)
View(par_grid)

#Final Model
gbm_3 <- gbm(formula = SalePrice ~ . ,
             distribution = "gaussian",
             data = train_1,
             n.trees = 800,
             interaction.depth = 5,
             shrinkage = 0.05,
             n.minobsinnode = 5,
             bag.fraction = 0.9,
             train.fraction = 1,
             cv.folds = 10,
             n.cores = NULL, #will use all cores by default
)  

summary(gbm_3)
#Relative Importance: 
#   The variables with the largest average decrease in MSE are considered most important.
#Test the Model----------------------------------
#Model 9: gbm_3
#Prediction
pred_gbm <- predict(gbm_3, n.trees = 800, newdata = test_1)
pred_gbm <- exp(pred_gbm)
summary(pred_gbm)
#Absolute error mean, median, sd, max, min-------
abs_err_gbm <- abs(pred_gbm - exp(test_1$SalePrice))
models_comp <- rbind(models_comp, "GBReg" = c(mean(abs_err_gbm),
                                              median(abs_err_gbm),
                                              sd(abs_err_gbm),
                                              IQR(abs_err_gbm),
                                              range(abs_err_gbm)))
View(models_comp)
#Actual vs. Predicted
plot(exp(test_1$SalePrice), pred_gbm, main = 'GBReg',
     #xlim = c(0, 50000), ylim = c(0, 500000),
     xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)

```

### **Test Kaggle test set**

```{r}
pred_gbm_k_1 <- predict(gbm_3, n.trees = 800, newdata = test_kaggle)
pred_gbm_k_1 <- exp(pred_gbm_k_1)
summary(pred_gbm_k_1)

pred_gbm_k_1 <- cbind("Id" = test$Id, "SalePrice" = pred_gbm_k_1)
write.csv(pred_gbm_k_1, file = "results/gbm_results.csv", row.names = FALSE)

#Save the results
save(gbm_3, file = "results/GBM_1.R")
```

### **Model 2: XGBoost Regression with Data 1**

XGBoost is another popular boosting algorithm known for its performance and speed. Here we build and tune XGBoost models to achieve the best possible predictive performance.

#### **Initial XGBoost Model**

```{r XGBoost Regression}
x <- model.matrix(SalePrice ~ . , 
                  data = train_1)[, -1] #remove intercept
y <- train_1$SalePrice

set.seed(724)
xgb_1 <- xgboost(data = x, 
                 label = y,
                 eta = 0.1,                       #learning rate
                 lambda = 0,                      #regularization term
                 max_depth = 8,                   #tree depth 
                 nround = 500,                   #max number of boosting iterations
                 subsample = 0.65,                #percent of training data to sample for each tree
                 objective = "reg:squarederror",  #for regression models
                 verbose = 0                      #silent
) 

#train RMSE

xgb_1$evaluation_log
#plot error vs number trees
ggplot(xgb_1$evaluation_log) +
  geom_line(aes(iter, train_rmse), color = "red") 

#Tuning(Train/validation using xgboost)
#Train and validation sets
set.seed(724)
train_cases <- sample(1 : nrow(train_1), nrow(train_1) * 0.8)
#Train data set
train_xgboost <- train_1[train_cases, ]
dim(train_xgboost)
#Model Matrix
xtrain <- model.matrix(SalePrice ~ . , 
                       data = train_xgboost)[, -1] #remove intercept
ytrain <- train_xgboost$SalePrice
#Validation data set
validation_xgboost  <- train_1[-train_cases,]
dim(validation_xgboost)
xvalidation <- model.matrix(SalePrice ~ . , 
                            data = validation_xgboost)[, -1] #remove intercept
yvalidation <- validation_xgboost$SalePrice

#Create hyper-parameter grid
par_grid <- expand.grid(eta = c(0.01, 0.05, 0.1, 0.3),
                        lambda = c(0, 1, 2, 5),
                        max_depth = c(1, 3, 5, 7),
                        subsample = c(0.65, 0.8, 1), 
                        colsample_bytree = c(0.8, 0.9, 1))
# view(par_grid)
dim(par_grid)

#Grid search 
for(i in 1 : nrow(par_grid) ) {
  set.seed(724)
  
  #train model
  xgb_tune <- xgboost(data =  xtrain,
                      label = ytrain,
                      eta = par_grid$eta[i],
                      max_depth = par_grid$max_depth[i],
                      subsample = par_grid$subsample[i],
                      colsample_bytree = par_grid$colsample_bytree[i],
                      nrounds = 1000,
                      objective = "reg:squarederror",  #for regression models
                      verbose = 0,                     #silent,
                      early_stopping_rounds = 10       #stop if no improvement for 10
                      #consecutive trees
                      )
  #prediction on validation data set
  pred_xgb_validation <- predict(xgb_tune, xvalidation)
  rmse <- sqrt(mean((yvalidation - pred_xgb_validation) ^ 2))
  
  #add validation error
  par_grid$RMSE[i]  <- rmse
}
save(par_grid, file = "results/par_grid_xgboost.R") #save results
load("results/par_grid_xgboost.R")                  #load results
View(par_grid)

#Final Model
set.seed(724)
xgb_2 <- xgboost(data = x, 
                 label = y,
                 eta = 0.05,     #learning rate
                 max_depth = 3,  #tree depth 
                 lambda = 0,
                 nround = 1000,
                 colsample_bytree = 0.9,
                 subsample = 0.8,                #percent of training data to sample for each tree
                 objective = "reg:squarederror",  #for regression models
                 verbose = 0,                      #silent
                 early_stopping_rounds = 10
)

#Test the Model----------------------------------
#Model 10: xgb_2
x_test   <- model.matrix(SalePrice ~ . , 
                         data = test_1)[, -1]#remove intercept
pred_xgb <- predict(xgb_2, x_test)
pred_xgb <- exp(pred_xgb)
summary(pred_xgb)
#Absolute error mean, median, sd, max, min-------
abs_err_xgb <- abs(pred_xgb - exp(test_1$SalePrice))
models_comp <- rbind(models_comp, "XGBReg" = c(mean(abs_err_xgb),
                                               median(abs_err_xgb),
                                               sd(abs_err_xgb),
                                               IQR(abs_err_xgb),
                                               range(abs_err_xgb)))
View(models_comp)
#Actual vs. Predicted
plot(exp(test_1$SalePrice), pred_xgb, main = 'XGBReg',
     xlim = c(0, 450000), ylim = c(0, 450000),
     xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
```

### **Test Kaggle test set**

```{r}
test_k_11 <- test_kaggle
test_k_11$SalePrice <- 0
x_test_k_1   <- model.matrix(SalePrice ~ . , 
                         data = test_k_11)[, -1]#remove intercept
pred_xgb_k_1 <- predict(xgb_2, x_test_k_1)
pred_xgb_k_1 <- exp(pred_xgb_k_1)
summary(pred_xgb_k_1)

pred_xgb_k_1 <- cbind("Id" = test$Id, "SalePrice" = pred_xgb_k_1)
write.csv(pred_gbm_k_1, file = "results/xgb_results.csv", row.names = FALSE)

#Save the results
save(xgb_2, file = "results/XGB_1.R")
```

```{r}
write.csv(models_comp, file = "results/models_com.csv", row.names = TRUE)
```

## Part 5: Evaluation

#### 1: Compare Models

We will compare the performance of the models across the two datasets based on the mean of absolute errors and the maximum of absolute errors.

**Mean of Absolute Errors:** Models with lower mean values (such as RandomForest_1 and Bagging_1) perform better on average.

**Median of Absolute Errors**: Again, models like RandomForest_1 and Bagging_1 exhibit lower medians, indicating better central tendency in their error distribution.

**Standard Deviation of Absolute Errors:** Lower values (e.g., in RandomForest_1 and LM_t-test_1) suggest that these models have more consistent errors.
    
**IQR of Absolute Errors:** This is another variability measure, where RandomForest_1 and Bagging_1 show lower IQRs, suggesting that the bulk of their errors are concentrated within a narrow range.

**Minimum of Absolute Errors:** This gives the smallest absolute error. Lower minimums are favorable, but extreme cases (like very low minimums) may not reflect overall model performance.

**Maximum of Absolute Errors:** Models with lower maximum absolute errors are generally preferred, as they show better handling of worst-case scenarios.

Based on this comparison, RandomForest_1 and Bagging_1 seem to perform consistently well across multiple metrics.