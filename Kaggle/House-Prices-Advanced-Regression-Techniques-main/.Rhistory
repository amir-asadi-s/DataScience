)
abline(a = 0, b = 1, col = "red", lwd = 2)
set.seed(724)
# mtry	for regression = p/3  ---> 14
rf_1 <- randomForest(SalePrice ~ . ,
data = train_1,
mtry = 14, ntree = 1000,
nodesize = 10, importance = TRUE
)
rf_1
importance(rf_1)
varImpPlot(rf_1)
#  remove "SalePrice"[53 ] & "log_SalePrice"[54]
colnames(train_1[, c(39)])
rf_cv_1 <- rfcv(train_1[, -c(39)],
train_1$SalePrice,
cv.fold = 10,
step = 0.9,
mtry = function(p) max(1, floor(sqrt(p))),
recursive = FALSE
)
class(rf_cv_1)
str(rf_cv_1)
# Vector of number of variables used at each step
rf_cv_1$n.var
# Corresponding vector of MSEs at each step
rf_cv_1$error.cv
which.min(rf_cv_1$error.cv)
# select 22 variables based on Importance of Variables
sort(importance(rf_1, )[, 1])
# Regression formula
reg_formula_1 <- as.formula(SalePrice ~  MSZoning  + Exterior1st +
CentralAir + BsmtFinType1 +
X2ndFlrSF + MSZoning + GarageCars +
Total_Bathrooms + YearRemodAdd + X1stFlrSF +
GarageArea + BsmtFinSF1 + LotArea +
YearBuilt + Neighborhood + GrLivArea +
OverallQual + TotalSF )
reg_formula_1
class(reg_formula_1)
# Tuning Hyperparameters
# Set random seed
set.seed(724)
# Define a function to perform random forest training with different hyperparameters
train_rf <- function(data, formula, mtry, ntree, nodesize) {
# Train random forest model
model <- randomForest(formula, data = data, mtry = mtry, ntree = ntree, nodesize = nodesize, importance = TRUE)
# Return the model and importance scores
return(list(model = model, importance = importance(model)))
}
# Define hyperparameter grid
mtry_grid <- c(floor(sqrt(ncol(train_1))), floor(ncol(train_1)/3))
ntree_grid <- c(500, 1000, 1500, 2000)
# Initialize empty list to store results
results <- list()
# Loop through each combination of mtry and ntree
for (i in 1:length(mtry_grid)) {
for (j in 1:length(ntree_grid)) {
# Current hyperparameter combination
current_mtry <- mtry_grid[i]
current_ntree <- ntree_grid[j]
# Train model with current hyperparameters
model_result <- train_rf(train_1, reg_formula_1, current_mtry, current_ntree, nodesize = 7)
# Store results in a list
results[[paste("mtry_", current_mtry, "_ntree_", current_ntree, sep = "")]] <- model_result
}
}
# Function to evaluate model performance (replace with your preferred metric)
evaluate_model <- function(model) {
# Example: Calculate mean squared error (MSE) on test data
predictions <- predict(model$model, test_1)
Rmse <- sqrt(mean((predictions - test_1$SalePrice)^2))
return(Rmse)
}
# Evaluate each model based on chosen metric
model_evaluations <- sapply(results, function(x) evaluate_model(x))
# Find the best model based on minimum evaluation score
best_model_index <- which.min(model_evaluations)
best_model_name <- names(model_evaluations)[best_model_index]
# Extract the best model and importance scores
best_model <- results[[best_model_name]]$model
best_importance <- results[[best_model_name]]$importance
# Use the best model for prediction
pred_rf_1 <- predict(best_model, test_1)
pred_rf_1 <- exp(pred_rf_1)
summary(pred_rf_1)
# Absolute error mean, median, sd, max, min-------
abs_err_rf_1 <- abs(pred_rf_1 - exp(test_1$SalePrice))
models_comp <- rbind(models_comp, "RandomForest_1" = c(
mean(abs_err_rf_1),
median(abs_err_rf_1),
sd(abs_err_rf_1),
IQR(abs_err_rf_1),
range(abs_err_rf_1)
))
View(models_comp)
models_comp
# Actual vs. Predicted
plot(exp(test_1$SalePrice), pred_rf_1,
main = "RandomForest_1",
xlab = "Actual", ylab = "Prediction"
)
abline(a = 0, b = 1, col = "red", lwd = 2)
# Use the best model for prediction
pred_rf_k_1 <- predict(best_model, test_kaggle)
pred_rf_k_1 <- exp(pred_rf_k_1)
sum(is.na(pred_rf_k_1))
pred_rf_k_1 <- cbind("Id" = test$Id, "SalePrice" = pred_rf_k_1)
write.csv(pred_rf_k_1, file = "results/pred_rf_k_1.csv", row.names = FALSE)
#Save the results
save(best_model, file = "results/RandomForest_1.R")
# Train GBM model with basic parameters
set.seed(724)
gbm_1 <- gbm(formula = SalePrice ~ . ,
distribution = "gaussian", #Gaussian for regression problems
data = train_1,
n.trees = 2000, #The total number of trees to fit
interaction.depth = 1, #1: stump, the maximum depth of each tree
shrinkage = 0.01, #Learning rate
cv.folds = 5,   #Number of cross-validation folds to perform
n.cores = NULL, #Use all cores by default
verbose = FALSE)
#Get MSE and compute RMSE
min(gbm_1$cv.error)         #MSE
sqrt(min(gbm_1$cv.error))   #RMSE
#plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm_1, method = "cv")
which(gbm_1$cv.error == min(gbm_1$cv.error))
# Train GBM model with different parameters
set.seed(724)
gbm_2 <- gbm(formula = SalePrice ~ . ,
distribution = "gaussian",
data = train_1,
n.trees = 200,
interaction.depth = 5,
shrinkage = 0.1,
cv.folds = 5,
n.cores = NULL, #will use all cores by default
verbose = FALSE)
#get MSE and compute RMSE
min(gbm_2$cv.error)
sqrt(min(gbm_2$cv.error))
#plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm_2, method = "cv")
which(gbm_2$cv.error == min(gbm_2$cv.error))
#Tuning
#Create hyper-parameter grid
par_grid <- expand.grid(shrinkage = c(0.01, 0.1, 0.3),  #learning rate
interaction_depth = c(5, 10, 15), #the maximum depth of each tree
n_minobsinnode = c(5, 10, 15),  #the minimum number of observations in the terminal nodes of the trees
bag_fraction = c(0.5, 0.7, 0.9) #stochastic gradient :bag.fraction < 1
)
# view(par_grid)
nrow(par_grid)
#Grid search (train/validation approach)
for(i in 1 : nrow(par_grid) ) {
set.seed(123)
#train model
gbm_tune <- gbm(formula = SalePrice ~ . ,
distribution = "gaussian",
data = train_1,
n.trees = 2000,
interaction.depth = par_grid$interaction_depth[i],
shrinkage = par_grid$shrinkage[i],
n.minobsinnode = par_grid$n_minobsinnode[i],
bag.fraction = par_grid$bag_fraction[i],
train.fraction = 0.8,
cv.folds = 0,
n.cores = NULL, #will use all cores by default
verbose = FALSE)
#add min training error and trees to grid
par_grid$optimal_trees[i] <- which.min(gbm_tune$valid.error)
par_grid$min_RMSE[i]      <- sqrt(min(gbm_tune$valid.error))
}
#Final Model
gbm_3 <- gbm(formula = SalePrice ~ . ,
distribution = "gaussian",
data = train_1,
n.trees = 800,
interaction.depth = 5,
shrinkage = 0.05,
n.minobsinnode = 5,
bag.fraction = 0.9,
train.fraction = 1,
cv.folds = 10,
n.cores = NULL, #will use all cores by default
)
summary(gbm_3)
#Relative Importance:
#   The variables with the largest average decrease in MSE are considered most important.
#Test the Model----------------------------------
#Model 9: gbm_3
#Prediction
pred_gbm <- predict(gbm_3, n.trees = 800, newdata = test_1)
pred_gbm <- exp(pred_gbm)
summary(pred_gbm)
#Absolute error mean, median, sd, max, min-------
abs_err_gbm <- abs(pred_gbm - exp(test_1$SalePrice))
models_comp <- rbind(models_comp, "GBReg" = c(mean(abs_err_gbm),
median(abs_err_gbm),
sd(abs_err_gbm),
IQR(abs_err_gbm),
range(abs_err_gbm)))
View(models_comp)
#Actual vs. Predicted
plot(exp(test_1$SalePrice), pred_gbm, main = 'GBReg',
#xlim = c(0, 50000), ylim = c(0, 500000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
pred_gbm_k_1 <- predict(gbm_3, n.trees = 800, newdata = test_kaggle)
pred_gbm_k_1 <- exp(pred_gbm_k_1)
summary(pred_gbm_k_1)
pred_gbm_k_1 <- cbind("Id" = test$Id, "SalePrice" = pred_gbm_k_1)
write.csv(pred_gbm_k_1, file = "results/gbm_results.csv", row.names = FALSE)
#Save the results
save(gbm_3, file = "results/GBM_1.R")
x <- model.matrix(SalePrice ~ . ,
data = train_1)[, -1] #remove intercept
y <- train_1$SalePrice
set.seed(724)
xgb_1 <- xgboost(data = x,
label = y,
eta = 0.1,                       #learning rate
lambda = 0,                      #regularization term
max_depth = 8,                   #tree depth
nround = 500,                   #max number of boosting iterations
subsample = 0.65,                #percent of training data to sample for each tree
objective = "reg:squarederror",  #for regression models
verbose = 0                      #silent
)
#train RMSE
xgb_1$evaluation_log
#plot error vs number trees
ggplot(xgb_1$evaluation_log) +
geom_line(aes(iter, train_rmse), color = "red")
#Tuning(Train/validation using xgboost)
#Train and validation sets
set.seed(724)
train_cases <- sample(1 : nrow(train_1), nrow(train_1) * 0.8)
#Train data set
train_xgboost <- train_1[train_cases, ]
dim(train_xgboost)
#Model Matrix
xtrain <- model.matrix(SalePrice ~ . ,
data = train_xgboost)[, -1] #remove intercept
ytrain <- train_xgboost$SalePrice
#Validation data set
validation_xgboost  <- train_1[-train_cases,]
dim(validation_xgboost)
xvalidation <- model.matrix(SalePrice ~ . ,
data = validation_xgboost)[, -1] #remove intercept
yvalidation <- validation_xgboost$SalePrice
#Final Model
set.seed(724)
xgb_2 <- xgboost(data = x,
label = y,
eta = 0.05,     #learning rate
max_depth = 3,  #tree depth
lambda = 0,
nround = 1000,
colsample_bytree = 0.9,
subsample = 0.8,                #percent of training data to sample for each tree
objective = "reg:squarederror",  #for regression models
verbose = 0,                      #silent
early_stopping_rounds = 10
)
#Test the Model----------------------------------
#Model 10: xgb_2
x_test   <- model.matrix(SalePrice ~ . ,
data = test_1)[, -1]#remove intercept
pred_xgb <- predict(xgb_2, x_test)
pred_xgb <- exp(pred_xgb)
summary(pred_xgb)
#Absolute error mean, median, sd, max, min-------
abs_err_xgb <- abs(pred_xgb - exp(test_1$SalePrice))
models_comp <- rbind(models_comp, "XGBReg" = c(mean(abs_err_xgb),
median(abs_err_xgb),
sd(abs_err_xgb),
IQR(abs_err_xgb),
range(abs_err_xgb)))
View(models_comp)
#Actual vs. Predicted
plot(exp(test_1$SalePrice), pred_xgb, main = 'XGBReg',
xlim = c(0, 450000), ylim = c(0, 450000),
xlab = "Actual", ylab = "Prediction")
abline(a = 0, b = 1, col = "red", lwd = 2)
test_k_11 <- test_kaggle
test_k_11$SalePrice <- 0
x_test_k_1   <- model.matrix(SalePrice ~ . ,
data = test_k_11)[, -1]#remove intercept
pred_xgb_k_1 <- predict(xgb_2, x_test_k_1)
pred_xgb_k_1 <- exp(pred_xgb_k_1)
summary(pred_xgb_k_1)
pred_xgb_k_1 <- cbind("Id" = test$Id, "SalePrice" = pred_xgb_k_1)
write.csv(pred_gbm_k_1, file = "results/xgb_results.csv", row.names = FALSE)
#Save the results
save(xgb_2, file = "results/XGB_1.R")
#Save the results
save(xgb_2, file = "results/XGB_1.R")
```{r}
write.csv(models_comp, file = "results/models_com.csv", row.names = TRUE)
# Mean of (mean_of_AbsErrors) based on three data set:
dim(models_comp)
# Create individual plots for each metric
# Mean of Absolute Errors
p1 <- ggplot(models_comp, aes(x = Model, y = Mean_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Mean of Absolute Errors", y = "Error", x = "Model")
# Load necessary libraries
library(ggplot2)
library(readr)
# Read the CSV file
data <- read_csv("path_to_file/models_com.csv")
# Rename the first column for convenience
colnames(models_comp)[1] <- "Model"
# Create individual plots for each metric
# Mean of Absolute Errors
p1 <- ggplot(models_comp, aes(x = Model, y = Mean_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Mean of Absolute Errors", y = "Error", x = "Model")
# Median of Absolute Errors
p2 <- ggplot(models_comp, aes(x = Model, y = Median_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Median of Absolute Errors", y = "Error", x = "Model")
# Standard Deviation of Absolute Errors
p3 <- ggplot(models_comp, aes(x = Model, y = SD_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Standard Deviation of Absolute Errors", y = "Error", x = "Model")
# IQR of Absolute Errors
p4 <- ggplot(models_comp, aes(x = Model, y = IQR_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "IQR of Absolute Errors", y = "Error", x = "Model")
# Minimum of Absolute Errors
p5 <- ggplot(models_comp, aes(x = Model, y = Min_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Minimum of Absolute Errors", y = "Error", x = "Model")
# Maximum of Absolute Errors
p6 <- ggplot(models_comp, aes(x = Model, y = Max_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Maximum of Absolute Errors", y = "Error", x = "Model")
# Display the plots using grid.arrange
library(gridExtra)
grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)
rownames(models_comp)[1]
# Rename the first column for convenience
rownames(models_comp)[0] <- "Model"
models_comp['PCR_1']
models_comp['PCR_1',]
rownames(models_comp)
# Rename the first column for convenience
# Change row names to a column called "Row"
models_comp <- models_comp %>% rownames_to_column("Model")
# Rename the first column for convenience
# Change row names to a column called "Row"
models_comp <- models_comp %>% dplyr::rownames_to_column("Model")
# Rename the first column for convenience
# Change row names to a column called "Row"
# Get the row names
row_names <- rownames(models_comp)
# Remove the row names from the dataframe
rownames(models_comp) <- NULL
# Add the row names as a new column
models_comp$Model <- row_names
# Load necessary libraries
library(ggplot2)
library(readr)
# Rename the first column for convenience
# Change row names to a column called "Row"
# Get the row names
row_names <- rownames(models_comp)
# Remove the row names from the dataframe
rownames(models_comp) <- NULL
# Add the row names as a new column
models_comp$Model <- row_names
# Create individual plots for each metric
# Mean of Absolute Errors
p1 <- ggplot(models_comp, aes(x = Model, y = Mean_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Mean of Absolute Errors", y = "Error", x = "Model")
# Median of Absolute Errors
p2 <- ggplot(models_comp, aes(x = Model, y = Median_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Median of Absolute Errors", y = "Error", x = "Model")
# Standard Deviation of Absolute Errors
p3 <- ggplot(models_comp, aes(x = Model, y = SD_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Standard Deviation of Absolute Errors", y = "Error", x = "Model")
# IQR of Absolute Errors
p4 <- ggplot(models_comp, aes(x = Model, y = IQR_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "IQR of Absolute Errors", y = "Error", x = "Model")
# Minimum of Absolute Errors
p5 <- ggplot(models_comp, aes(x = Model, y = Min_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Minimum of Absolute Errors", y = "Error", x = "Model")
# Maximum of Absolute Errors
p6 <- ggplot(models_comp, aes(x = Model, y = Max_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Maximum of Absolute Errors", y = "Error", x = "Model")
# Display the plots using grid.arrange
library(gridExtra)
grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)
data <- read_csv("results/models_com.csv")
models_com <- read_csv("results/models_com.csv")
# Rename the first column for convenience
# Change row names to a column called "Row"
# Get the row names
row_names <- rownames(models_comp)
# Remove the row names from the dataframe
rownames(models_comp) <- NULL
# Add the row names as a new column
models_comp$Model <- row_names
models_comp <- read_csv("results/models_com.csv")
# Rename the first column for convenience
# Change row names to a column called "Row"
# Get the row names
row_names <- rownames(models_comp)
# Remove the row names from the dataframe
rownames(models_comp) <- NULL
# Add the row names as a new column
models_comp$Model <- row_names
# Create individual plots for each metric
# Mean of Absolute Errors
p1 <- ggplot(models_comp, aes(x = Model, y = Mean_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Mean of Absolute Errors", y = "Error", x = "Model")
# Median of Absolute Errors
p2 <- ggplot(models_comp, aes(x = Model, y = Median_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Median of Absolute Errors", y = "Error", x = "Model")
# Standard Deviation of Absolute Errors
p3 <- ggplot(models_comp, aes(x = Model, y = SD_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Standard Deviation of Absolute Errors", y = "Error", x = "Model")
# IQR of Absolute Errors
p4 <- ggplot(models_comp, aes(x = Model, y = IQR_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "IQR of Absolute Errors", y = "Error", x = "Model")
# Minimum of Absolute Errors
p5 <- ggplot(models_comp, aes(x = Model, y = Min_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Minimum of Absolute Errors", y = "Error", x = "Model")
# Maximum of Absolute Errors
p6 <- ggplot(models_comp, aes(x = Model, y = Max_of_AbsErrors)) +
geom_bar(stat = "identity", fill = "steelblue") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Maximum of Absolute Errors", y = "Error", x = "Model")
# Display the plots using grid.arrange
library(gridExtra)
grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)
# Create individual bar plots
barplot(models_comp$Mean_of_AbsErrors, names.arg = models_comp$Model, main = "Mean of Absolute Errors", ylab = "Error", xlab = "Model")
# Load necessary libraries
library(ggplot2)
library(readr)
models_comp <- read_csv("results/models_com.csv")
# Rename the first column for convenience
# Change row names to a column called "Row"
# Get the row names
row_names <- rownames(models_comp)
# Remove the row names from the dataframe
rownames(models_comp) <- NULL
# Add the row names as a new column
models_comp$Model <- row_names
# Set up the plot layout
par(mfrow = c(3, 2))  # 3 rows, 2 columns
# Create individual bar plots
barplot(models_comp$Mean_of_AbsErrors, names.arg = models_comp$Model, main = "Mean of Absolute Errors", ylab = "Error", xlab = "Model")
# Set up the plot layout
par(mfrow = c(2, 3))  # 3 rows, 2 columns
# Create individual bar plots
barplot(models_comp$Mean_of_AbsErrors, names.arg = models_comp$Model, main = "Mean of Absolute Errors", ylab = "Error", xlab = "Model")
barplot(models_comp$Median_of_AbsErrors, names.arg = models_comp$Model, main = "Median of Absolute Errors", ylab = "Error", xlab = "Model")
barplot(models_comp$SD_of_AbsErrors, names.arg = models_comp$Model, main = "Standard Deviation of Absolute Errors", ylab = "Error", xlab = "Model")
barplot(models_comp$IQR_of_AbsErrors, names.arg = models_comp$Model, main = "IQR of Absolute Errors", ylab = "Error", xlab = "Model")
barplot(models_comp$Min_of_AbsErrors, names.arg = models_comp$Model, main = "Minimum of Absolute Errors", ylab = "Error", xlab = "Model")
barplot(models_comp$Max_of_AbsErrors, names.arg = models_comp$Model, main = "Maximum of Absolute Errors", ylab = "Error", xlab = "Model")
# Create individual bar plots
# Set up the plot layout
par(mfrow = c(3, 2))
# Create individual bar plots with customized labels
barplot(models_comp$Mean_of_AbsErrors, main = "Mean of Absolute Errors", ylab = "Error", xlab = "Model")
axis(side = 1, at = seq_along(models_comp$Model), labels = models_comp$Model, las = 2)
barplot(models_comp$Median_of_AbsErrors, main = "Median of Absolute Errors", ylab = "Error", xlab = "Model")
# Create individual bar plots
# Set up the plot layout
par(mfrow = c(3, 2))
# Create individual bar plots
# Set up the plot layout
par(mfrow = c(2, 3))
# Create individual bar plots with customized labels
barplot(models_comp$Mean_of_AbsErrors, main = "Mean of Absolute Errors", ylab = "Error", xlab = "Model")
axis(side = 1, at = seq_along(models_comp$Model), labels = models_comp$Model, las = 2)
par(mfrow = c(1, 1))
