{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport itertools\nimport time\nfrom scipy.stats import iqr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Load Data","metadata":{}},{"cell_type":"code","source":"# -----------------\n# Load the training and test datasets\ndf_train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\n# For the test dataset, add the target column 'SalePrice' with value 0 (for Combine train and test data for preprocessing)\nif 'SalePrice' not in df_test.columns:\n    df_test['SalePrice'] = 0\n    \n# Combine train and test data for preprocessing\ndf = pd.concat([df_train, df_test], axis = 0)\ndf = df.set_index('Id')\n\n# Display the first few rows of the combined dataset\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Handle Missing Values","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# Find missing values across all columns\ndf_mv  = df[df.isnull().sum()[df.isnull().sum()>0].index]\n\n# Visualize missing values using a heatmap\nsns.heatmap(df_mv.isnull())\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove categorical columns with more than 1100 missing values\ndf_objects =  df[df.select_dtypes(include=['object']).columns]\ndf = df.drop(df[df_objects.isna().sum()[df_objects.isna().sum() > 1100].index], axis = 1)\n\n# Fill missing values of other categorical columns with 'MV' and encode them using one-hot encoding\ndf_objects = df_objects.drop(df_objects[df_objects.isna().sum()[df_objects.isna().sum() > 1100].index], axis = 1)\ndf_objects = df_objects.fillna('MV')\ndf_objects_encoded = pd.get_dummies(df_objects)\n\n# Drop columns that are encoded as 'MV'\nfor i in df_objects_encoded.columns:\n    if 'MV' in i:\n        df_objects_encoded = df_objects_encoded.drop(i, axis = 1)\n        print(i)\n        \n# Combine the original dataframe with the encoded categorical columns\nnew_df = pd.concat([df, df_objects_encoded], axis = 1)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Handle Missing Values in Numerical Columns","metadata":{}},{"cell_type":"markdown","source":"#### impute the missing values of numerical values","metadata":{}},{"cell_type":"code","source":"# Drop any remaining object-type columns (they've already been encoded)\nnew_df = new_df.drop(df.select_dtypes(include=['object']), axis = 1)\n\n#get an overview of numerical missing values\nnew_df.isna().sum()[new_df.isna().sum() > 0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute missing values in numerical columns\n# Use mode for ordinal features and mean for continuous features\n\nMode_columns = ['GarageCars', 'GarageYrBlt', 'BsmtFullBath', 'BsmtHalfBath']\nMean_columns = ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n                'TotalBsmtSF', 'GarageArea']\n\nfor i in Mode_columns:\n    new_df[i] = new_df[i].fillna(new_df[i].mode()[0])\n\nfor i in Mean_columns:\n    new_df[i] = new_df[i].fillna(np.round(new_df[i].mean()))\n\n# check for removing all missing values\nnew_df.isna().sum()[new_df.isna().sum() > 0] ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Data Splitting","metadata":{}},{"cell_type":"code","source":"# ----------------------\n# Separate the train and test sets from the combined dataset\ntrain_data = new_df[0:len(df_train)]\ntest_data = new_df[len(df_train):]\ntest_data = test_data.drop(columns='SalePrice')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Target Variable Analysis (SalePrice)","metadata":{}},{"cell_type":"code","source":"# --------------------------------------------\n# Visualize the distribution of SalePrice\nsns.histplot(data=train_data, x=\"SalePrice\", color=\"red\", kde=True, bins=30)\nplt.title(\"Sale Price Histogram\")\nplt.xlabel(\"Sale Price\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Display SalePrice summary statistics\nprint(train_data[\"SalePrice\"].describe())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply Box-Cox transformation to SalePrice to normalize its distribution\nfrom sklearn.preprocessing import PowerTransformer\ny_train = train_data['SalePrice']\nboxcox = PowerTransformer(method = 'box-cox')\nboxcox.fit(y_train.values.reshape(-1, 1))\ntrans_y_train = boxcox.fit_transform(y_train.values.reshape(-1, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the transformed SalePrice distribution\nsns.histplot(trans_y_train, color=\"red\", kde=True, bins=30)\nplt.title(\"Sale Price Histogram\")\nplt.xlabel(\"Sale Price\")\nplt.ylabel(\"Count\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write preprocessed data to a CSV file for further analysis\ntrain_data.to_csv('train_data.csv', index=True)\ntest_data.to_csv('test_data.csv', index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Split Data into Training and Validation Sets","metadata":{}},{"cell_type":"code","source":"# ----------------------------------------------------\n#Split data into test and train\ntrain, test = train_test_split(train_data, \n                               test_size = 0.3, \n                               random_state = 724)\nprint(train.shape)\nprint(test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define feature matrices and target vectors for train set\nX_train = train.drop(['SalePrice'], axis = 1)\nX_train.head()\n\ny_train = train['SalePrice']\ny_train\n\n#Box-Cox transformation for target variable in train set\nfrom sklearn.preprocessing import PowerTransformer\nboxcox = PowerTransformer(method = 'box-cox')\nboxcox.fit(y_train.values.reshape(-1, 1))\ntrans_y_train = boxcox.fit_transform(y_train.values.reshape(-1, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define feature matrices and target vectors for validation set\nX_test = test.drop(['SalePrice'], axis = 1)\nX_test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 7: Train Random Forest Model","metadata":{}},{"cell_type":"code","source":"# ---------------------------------\n# Initialize and train a Random Forest Regressor\nrf_reg = RandomForestRegressor(n_estimators = 2000,\n                               max_features = X_train.shape[1], \n                               criterion = 'squared_error',\n                               max_depth = 20, \n                               min_samples_leaf = 4, \n                               ccp_alpha = 0,\n                               random_state = 724)\n\nmodel_1 = rf_reg.fit(X_train, trans_y_train.reshape(-1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate feature importance\nimportance = pd.DataFrame({'importance': model_1.feature_importances_ * 100}, \n                          index = X_train.columns)\nfiltered_importance = importance[importance['importance'] > 1]\nfiltered_importance.sort_values(by = 'importance', axis = 0, ascending = True).plot(kind = 'barh',color = 'r')\nplt.title('Variable Importance')\nplt.xlabel('MSE Increase (%)')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter Tuning using Cross-Validation for RandomForest\n# ----------------------------------------------------\n# Create hyperparameter grid for Random Forest\nn_estimators = [2000, 2100]\nmax_features = ['sqrt', 'log2', None] #If None or 1.0, then max_features = n_features\nmax_depth = [20, 25]\nmin_samples_leaf = [4, 5, 6]\nparams_grid = list(itertools.product(n_estimators, max_features, max_depth, min_samples_leaf))\nparams_grid = pd.DataFrame(data = params_grid,\n                           index = range(1, 37), \n                           columns = ['n_estimators', \n                                      'max_features', \n                                      'max_depth', \n                                      'min_samples_leaf'])\nparams_grid","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform 5-fold cross-validation to choose the best hyperparameter\nstart_time = time.time()\ncv_errors = np.zeros(shape = len(params_grid)) #to save cv results\nfor i in range(len(params_grid)):\n    rf_reg = RandomForestRegressor(n_estimators = params_grid.iloc[i, 0],\n                                   max_features = params_grid.iloc[i, 1], \n                                   criterion = 'squared_error',\n                                   max_depth = params_grid.iloc[i, 2], \n                                   min_samples_leaf = params_grid.iloc[i, 3], \n                                   ccp_alpha = 0,\n                                  random_state= 42)\n    scores = cross_val_score(estimator = rf_reg, \n                             X = X_train, \n                             y = trans_y_train,\n                             scoring = 'neg_root_mean_squared_error',\n                             cv = 5, n_jobs = -1)\n    cv_errors[i] = scores.mean() \nend_time = time.time()\nprint('The Processing time is: ', end_time - start_time, 'seconds')\n\ncv_errors","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the best model\nbest_params = params_grid.iloc[np.argmax(cv_errors), :]\nprint(f'Best Random Forest parameters: {best_params}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrain the Random Forest model with the best parameters\nrf_reg = RandomForestRegressor(n_estimators = params_grid.iloc[np.argmax(cv_errors), 0],\n                               max_features = params_grid.iloc[np.argmax(cv_errors), 1], \n                               criterion = 'squared_error',\n                               max_depth = params_grid.iloc[np.argmax(cv_errors), 2], \n                               min_samples_leaf = params_grid.iloc[np.argmax(cv_errors), 3], \n                               ccp_alpha = 0,\n                               random_state = 42)\nmodel_1 = rf_reg.fit(X_train, trans_y_train.reshape(-1))\n\n#Prediction using model 1\npred_rf = model_1.predict(X_test)\npred_rf = pd.Series(boxcox.inverse_transform(pred_rf.reshape(-1, 1)).reshape(-1), \n                    index = test.index)\npred_rf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Absolute error\nabs_err_rf = abs(test['SalePrice'] - pred_rf)\n\n#Absolute error mean, median, sd, IQR, max, min\nfrom scipy.stats import iqr\nmodels_comp = pd.DataFrame({'Mean of AbsErrors':    abs_err_rf.mean(),\n                                       'Median of AbsErrors' : abs_err_rf.median(),\n                                       'SD of AbsErrors' :     abs_err_rf.std(),\n                                       'IQR of AbsErrors':     iqr(abs_err_rf),\n                                       'Min of AbsErrors':     abs_err_rf.min(),\n                                       'Max of AbsErrors':     abs_err_rf.max()}, \n                                      index = ['Random Forest'])\nmodels_comp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Actual vs. Prediction\nplt.scatter(x = test['SalePrice'], y = pred_rf, c = 'black', alpha = 0.7)\nplt.xlabel('Actual')\nplt.ylabel('Prediction')\nplt.title('Actual vs. Prediction - Random Forest')\n\n#Add 45 degree line\nxp = np.linspace(test['SalePrice'].min(), test['SalePrice'].max(), 100)\nplt.plot(xp, xp, c = 'red', linewidth = 3)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 8: Gradient Boosting Regressor","metadata":{}},{"cell_type":"code","source":"# -----------------------------------\n# Initialize and train a Gradient Boosting Regressor (not tuned)\nboosting_reg = GradientBoostingRegressor(learning_rate = 0.05,  #learning rate\n                                         n_estimators = 300,   #the total number of trees to fit\n                                         subsample = 0.7,      #the fraction of samples to be used,  \n                                                                    #if .< 1, Stochastic GB\n                                         max_depth = 5,        #the maximum depth of each tree\n                                         min_samples_leaf = 16, #the minimum number of observations in the leaf nodes of the trees\n                                         random_state = 42)\n\nmodel_2 = boosting_reg.fit(X_train, trans_y_train.reshape(-1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter Tuning using Cross-Validation for GradientBoosting\n# ----------------------------------------------------\n# Create hyperparameter grid\nlearning_rate = [0.04, 0.05, 0.06 ]\nn_estimators = [250, 300, 350]\nsubsample = [0.6, 0.7, 0.9]\nmax_depth = [3, 4, 5]\nmin_samples_leaf = [4, 8, 16]\n\nparams_grid = list(itertools.product(learning_rate, n_estimators, subsample, max_depth, min_samples_leaf))\nparams_grid = pd.DataFrame(data = params_grid,\n                           index = range(1,244), \n                           columns = ['learning_rate',\n                                      'n_estimators', \n                                      'subsample', \n                                      'max_depth', \n                                      'min_samples_leaf'])\nparams_grid","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#K-fold cross validation to choose the best model\n\nstart_time = time.time()\ncv_errors = np.zeros(shape = len(params_grid)) #to save cv results\nfor i in range(len(params_grid)):\n    gb_reg = GradientBoostingRegressor(learning_rate = params_grid.iloc[i, 0],\n                                       n_estimators = params_grid.iloc[i, 1], \n                                       subsample = params_grid.iloc[i, 2], \n                                       max_depth = params_grid.iloc[i, 3],\n                                       min_samples_leaf = params_grid.iloc[i, 4],\n                                       random_state = 42)\n    scores = cross_val_score(estimator = gb_reg, \n                             X = X_train, \n                             y = trans_y_train,\n                             scoring = 'neg_root_mean_squared_error',\n                             cv = 5, n_jobs = -1)\n    cv_errors[i] = scores.mean() \nend_time = time.time()\nprint('The Processing time is: ', end_time - start_time, 'seconds')\n\ncv_errors","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the best model hyperparameters\nbest_params = params_grid.iloc[np.argmax(cv_errors), :]\nprint(f'Best Random Forest parameters: {best_params}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrain model 2 with best hyperparmeters\nboosting_reg = GradientBoostingRegressor(\n    learning_rate = params_grid.iloc[np.argmax(cv_errors), 0],\n    n_estimators = params_grid.iloc[np.argmax(cv_errors), 1],\n    subsample = params_grid.iloc[np.argmax(cv_errors), 2],\n    max_depth= params_grid.iloc[np.argmax(cv_errors), 3],\n    min_samples_leaf = params_grid.iloc[i, 4],\n    random_state = 42)\n    \nmodel_2 = boosting_reg.fit(X_train, trans_y_train.reshape(-1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prediction using model 2\npred_gbr = model_2.predict(X_test)\npred_gbr = pd.Series(boxcox.inverse_transform(pred_gbr.reshape(-1, 1)).reshape(-1), \n                    index = test.index)\npred_gbr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Absolute error\nabs_err_gbr = abs(test['SalePrice'] - pred_gbr)\n\n#Absolute error mean, median, sd, IQR, max, min\nfrom scipy.stats import iqr\nmodels_comp = pd.concat([models_comp,\n                         pd.DataFrame({'Mean of AbsErrors':    abs_err_gbr.mean(),\n                                       'Median of AbsErrors' : abs_err_gbr.median(),\n                                       'SD of AbsErrors' :     abs_err_gbr.std(),\n                                       'IQR of AbsErrors':     iqr(abs_err_gbr),\n                                       'Min of AbsErrors':     abs_err_gbr.min(),\n                                       'Max of AbsErrors':     abs_err_gbr.max()}, \n                                      index = ['GB Regressor'])])\nmodels_comp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Actual vs. Prediction\nplt.scatter(x = test['SalePrice'], y = pred_gbr, c = 'black', alpha = 0.7)\nplt.xlabel('Actual')\nplt.ylabel('Prediction')\nplt.title('Actual vs. Prediction - Gradient Boost Regressor')\n\n#Add 45 degree line\nxp = np.linspace(test['SalePrice'].min(), test['SalePrice'].max(), 100)\nplt.plot(xp, xp, c = 'red', linewidth = 3)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Step: prediction on test set and create submission file","metadata":{}},{"cell_type":"code","source":"pred = model_2.predict(test_data)\n\npred = pd.Series(boxcox.inverse_transform(pred.reshape(-1, 1)).reshape(-1),\n                 index = test_data.index)\n\nfinal = pd.DataFrame()\nfinal['Id'] = pred.index\nfinal['SalePrice'] = pred.tolist()\n\n# Write DataFrame to a CSV file without index\nfinal.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final","metadata":{},"execution_count":null,"outputs":[]}]}